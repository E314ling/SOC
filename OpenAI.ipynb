{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import gym\n",
    "from keras import layers, optimizers, losses\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class experience_memory():\n",
    "\n",
    "    def __init__(self, buffer_capacity, batch_size, state_dim, action_dim):\n",
    "        # Number of \"experiences\" to store at max\n",
    "        self.buffer_capacity = buffer_capacity\n",
    "        # Num of tuples to train on.\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # Its tells us num of times record() was called.\n",
    "        self.buffer_counter = 0\n",
    "\n",
    "        # Instead of list of tuples as the exp.replay concept go\n",
    "        # We use different np.arrays for each tuple element\n",
    "        self.state_buffer = np.zeros((self.buffer_capacity, state_dim), dtype=np.float32)\n",
    "        self.action_buffer = np.zeros((self.buffer_capacity, action_dim), dtype=np.float32)\n",
    "        self.reward_buffer = np.zeros((self.buffer_capacity, 1), dtype=np.float32)\n",
    "        self.next_state_buffer = np.zeros((self.buffer_capacity, state_dim), dtype=np.float32)\n",
    "        self.done_buffer = np.zeros((self.buffer_capacity, 1), dtype=np.float32)\n",
    "\n",
    "    # Takes (s,a,r,s') obervation tuple as input\n",
    "    def record(self, obs_tuple):\n",
    "        # Set index to zero if buffer_capacity is exceeded,\n",
    "        # replacing old records\n",
    "        index = self.buffer_counter % self.buffer_capacity\n",
    "\n",
    "        self.state_buffer[index] = obs_tuple[0]\n",
    "        self.action_buffer[index] = obs_tuple[1]\n",
    "        self.reward_buffer[index] = obs_tuple[2]\n",
    "        self.next_state_buffer[index] = obs_tuple[3]\n",
    "        self.done_buffer[index] = obs_tuple[4]\n",
    "\n",
    "\n",
    "        self.buffer_counter += 1\n",
    "\n",
    "class MountainCar():\n",
    "\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "        self.batch_size = 64\n",
    "        self.max_memory_size = 10000\n",
    "\n",
    "        self.state_dim = 4\n",
    "        self.action_dim = 1\n",
    "\n",
    "        self.gamma = 0.8\n",
    "        self.tau = 0.1\n",
    "        self.lower_action_bound = -1\n",
    "        self.upper_action_bound = 1\n",
    "\n",
    "        self.action_space = np.array([0,1])\n",
    "        self.num_a = len(self.action_space)\n",
    "\n",
    "        self.buffer = experience_memory(self.max_memory_size, self.batch_size, self.state_dim, self.action_dim)\n",
    "\n",
    "        # init the neural netsf\n",
    "        self.critic = self.get_critic_NN()\n",
    "        self.target_critic = self.get_critic_NN()\n",
    "        self.target_critic.set_weights(self.critic.get_weights())\n",
    "        \n",
    "        self.alpha = 0.1\n",
    "        self.critic_optimizer = tf.keras.optimizers.Adam(self.alpha)\n",
    "\n",
    "    #@tf.function\n",
    "    def update(self, state_batch, action_batch, reward_batch, next_state_batch, done_batch):\n",
    "             \n",
    "    \n",
    "        with tf.GradientTape() as tape:\n",
    "            \n",
    "            next_q_vals =  self.target_critic(next_state_batch, training=True)\n",
    "            \n",
    "            target_vals = tf.reshape(tf.reduce_max(next_q_vals, axis =1),[self.batch_size,1])\n",
    "            y = reward_batch + (1-done_batch)* self.gamma*target_vals\n",
    "           \n",
    "            critic_value = self.critic(state_batch, training=True)\n",
    "            \n",
    "            mask = tf.ones((self.batch_size, self.num_a))\n",
    "            critic_value[:, action_batch].assign(critic_value[:,action_batch] - y)\n",
    "            y = tf.multiply(mask, y)\n",
    "            \n",
    "            dif = tf.add(y,-critic_value)\n",
    "           \n",
    "            dif = tf.reduce_sum(dif,axis = 0)\n",
    "            \n",
    "            critic_loss = tf.reduce_mean(tf.math.square(dif))\n",
    "        \n",
    "        \n",
    "        critic_grad = tape.gradient(critic_loss, self.critic.trainable_variables)\n",
    "        self.critic_optimizer.apply_gradients(zip(critic_grad, self.critic.trainable_variables))\n",
    "\n",
    "        \n",
    "    def learn(self):\n",
    "        # get sample\n",
    "\n",
    "        record_range = min(self.buffer.buffer_counter, self.buffer.buffer_capacity)\n",
    "\n",
    "        batch_indices = np.random.choice(record_range, self.batch_size)\n",
    "        \n",
    "        state_batch = tf.convert_to_tensor(self.buffer.state_buffer[batch_indices])\n",
    "        action_batch = tf.convert_to_tensor(self.buffer.action_buffer[batch_indices])\n",
    "        reward_batch = tf.convert_to_tensor(self.buffer.reward_buffer[batch_indices])\n",
    "        next_state_batch = tf.convert_to_tensor(self.buffer.next_state_buffer[batch_indices])\n",
    "        done_batch = tf.convert_to_tensor(self.buffer.done_buffer[batch_indices])\n",
    "        \n",
    "        self.update(state_batch, action_batch, reward_batch, next_state_batch,done_batch)\n",
    "\n",
    "    @tf.function\n",
    "    def update_target(self, target_weights, weights):\n",
    "        for (a,b) in zip(target_weights, weights):\n",
    "            a.assign(self.tau *b + (1-self.tau) *a)\n",
    "\n",
    "    def get_critic_NN(self):\n",
    "        # input [state, action]\n",
    "        \n",
    "        state_input = layers.Input(shape =(self.state_dim,))\n",
    "\n",
    "        out = layers.Dense(32, activation = 'relu')(state_input)\n",
    "        out = layers.BatchNormalization()(out)\n",
    "        out = layers.Dense(32, activation = 'relu')(out)\n",
    "        out = layers.Dense(self.num_a)(out)\n",
    "\n",
    "        \n",
    "        model = keras.Model(inputs = state_input, outputs = out)\n",
    "        model.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(learning_rate= 0.001),\n",
    "            loss=tf.keras.losses.MeanSquaredError(),\n",
    "            metrics=[tf.keras.metrics.MeanSquaredError()],\n",
    "        )\n",
    "        return model\n",
    "\n",
    "    def epsilon_greedy(self, state, eps):\n",
    "\n",
    "        q_vals = self.critic(state)\n",
    "        \n",
    "        if (eps > np.random.rand()):\n",
    "           \n",
    "            rand_ind = np.random.choice(self.num_a)\n",
    "            \n",
    "            return self.action_space[rand_ind]\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            a_ind = tf.argmax(q_vals,axis = 1)\n",
    "           \n",
    "            return self.action_space[a_ind]\n",
    "    \n",
    "    def run_MC(self):\n",
    "        done = False\n",
    "        state = self.env.reset()\n",
    "        state = tf.expand_dims(tf.convert_to_tensor(state),0)\n",
    "        t_counter = 0\n",
    "        while (True):\n",
    "            self.env.render()\n",
    "            time.sleep(0.01)\n",
    "            \n",
    "            a_ind = np.argmax(self.critic(state))\n",
    "            \n",
    "            action = self.action_space[a_ind]\n",
    "            new_state, reward, done, info = self.env.step(action)\n",
    "        \n",
    "            new_state = tf.expand_dims(tf.convert_to_tensor(new_state.reshape(self.state_dim)),0)\n",
    "            state = new_state\n",
    "            t_counter += 1\n",
    "\n",
    "            if (done):\n",
    "                break\n",
    "        print('Zeit: ', t_counter)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No gradients provided for any variable: (['dense_144/kernel:0', 'dense_144/bias:0', 'batch_normalization_48/gamma:0', 'batch_normalization_48/beta:0', 'dense_145/kernel:0', 'dense_145/bias:0', 'dense_146/kernel:0', 'dense_146/bias:0'],). Provided `grads_and_vars` is ((None, <tf.Variable 'dense_144/kernel:0' shape=(4, 32) dtype=float32, numpy=\narray([[-4.00936395e-01, -2.73486882e-01,  1.22568667e-01,\n         6.96156919e-02, -9.44505632e-02,  4.23667431e-02,\n        -1.74931169e-01,  2.66924918e-01,  3.07104945e-01,\n        -3.51616383e-01,  1.83020175e-01,  2.73801386e-01,\n         3.20230424e-01, -2.18223467e-01,  2.97599316e-01,\n        -4.06813115e-01, -1.49141848e-02, -2.08690673e-01,\n        -2.18872100e-01,  2.55886197e-01,  3.06799531e-01,\n         2.89450407e-01, -3.70396882e-01, -1.94990143e-01,\n         1.26841724e-01,  1.57133043e-01, -4.00922060e-01,\n        -2.48249635e-01,  3.04789484e-01,  4.02402043e-01,\n        -3.99610877e-01,  1.71460986e-01],\n       [-2.66813934e-01,  3.51128161e-01,  2.45302677e-01,\n         2.66947091e-01, -2.34754965e-01, -2.01329798e-01,\n        -2.44265169e-01,  2.25244284e-01,  3.09244931e-01,\n         2.17825055e-01,  1.72226548e-01, -3.07151884e-01,\n        -2.22975209e-01,  1.00005925e-01, -3.86716276e-01,\n        -8.73490572e-02,  4.01823103e-01, -2.74759322e-01,\n         1.46090448e-01,  1.93512201e-01, -4.81333137e-02,\n         8.66463184e-02, -9.58945155e-02, -1.44664079e-01,\n        -3.33165556e-01,  1.54869556e-01, -2.29780897e-01,\n         3.05997729e-02, -7.59783089e-02, -3.94917250e-01,\n        -1.47163868e-04,  1.72646999e-01],\n       [-3.97770017e-01,  2.12070704e-01,  3.85072708e-01,\n        -2.50706851e-01, -1.39537305e-01,  1.48036003e-01,\n        -9.17261839e-02,  4.97787297e-02, -2.68852800e-01,\n        -9.83342826e-02, -6.56511784e-02, -1.63328856e-01,\n        -1.84429884e-01, -2.51028419e-01, -8.40532482e-02,\n        -4.41760123e-02,  1.04923487e-01, -2.40390599e-02,\n         2.48792291e-01, -1.93467438e-01, -2.64415443e-01,\n        -1.35006607e-01, -1.25621974e-01, -4.03649479e-01,\n        -2.01423332e-01, -9.32657123e-02, -2.92053044e-01,\n         7.21679032e-02,  1.28977954e-01, -1.46484435e-01,\n        -8.31231475e-03, -3.72183621e-02],\n       [ 1.68977678e-02,  1.43736303e-01,  2.65276730e-01,\n        -3.69222045e-01,  2.20938981e-01, -3.70237529e-01,\n        -1.32144302e-01, -2.54301488e-01, -3.36276054e-01,\n         6.96541369e-02, -1.19539082e-01, -2.78853476e-01,\n         1.85334206e-01,  3.87022197e-01,  1.11860394e-01,\n        -1.34776711e-01,  5.81405163e-02,  1.68263674e-01,\n        -1.52286708e-01,  2.69138217e-01,  2.48173714e-01,\n        -7.60022700e-02, -1.23761803e-01,  5.62369525e-02,\n        -1.13178581e-01, -2.34067976e-01,  2.97794282e-01,\n        -1.52031302e-01, -3.35489303e-01, -1.84138268e-01,\n        -6.88560009e-02, -3.40509802e-01]], dtype=float32)>), (None, <tf.Variable 'dense_144/bias:0' shape=(32,) dtype=float32, numpy=\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n      dtype=float32)>), (None, <tf.Variable 'batch_normalization_48/gamma:0' shape=(32,) dtype=float32, numpy=\narray([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n      dtype=float32)>), (None, <tf.Variable 'batch_normalization_48/beta:0' shape=(32,) dtype=float32, numpy=\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n      dtype=float32)>), (None, <tf.Variable 'dense_145/kernel:0' shape=(32, 32) dtype=float32, numpy=\narray([[ 0.27584746, -0.06496839, -0.09250213, ...,  0.0032478 ,\n         0.09209427,  0.05609125],\n       [ 0.26899102, -0.18551037,  0.27240524, ...,  0.0933547 ,\n         0.1751354 , -0.06559962],\n       [ 0.25935897,  0.06988373,  0.01142356, ..., -0.2896401 ,\n         0.10134986, -0.04356027],\n       ...,\n       [-0.1810588 , -0.24481012,  0.10682935, ..., -0.19621429,\n         0.28625175, -0.2935816 ],\n       [ 0.2693306 , -0.11832605,  0.09773582, ..., -0.26795778,\n        -0.15686703,  0.29578075],\n       [-0.16326559, -0.12623784, -0.0426068 , ...,  0.19628242,\n        -0.21311536,  0.29363546]], dtype=float32)>), (None, <tf.Variable 'dense_145/bias:0' shape=(32,) dtype=float32, numpy=\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n      dtype=float32)>), (None, <tf.Variable 'dense_146/kernel:0' shape=(32, 2) dtype=float32, numpy=\narray([[-0.07349417, -0.08894059],\n       [-0.3475104 , -0.09511098],\n       [ 0.27645335,  0.32201687],\n       [ 0.25151494, -0.35627246],\n       [-0.04199424, -0.33878264],\n       [-0.40793544, -0.3201866 ],\n       [ 0.3453938 ,  0.05230099],\n       [ 0.14364138,  0.11987033],\n       [ 0.2981361 ,  0.00301608],\n       [-0.18693806, -0.22724429],\n       [-0.01885152,  0.0340575 ],\n       [ 0.3465357 ,  0.00538707],\n       [ 0.34752986, -0.05299577],\n       [ 0.3433369 ,  0.26623127],\n       [ 0.2642022 , -0.09142986],\n       [ 0.37769786,  0.02710146],\n       [-0.24965996, -0.29034677],\n       [ 0.21406886, -0.04898313],\n       [ 0.24426469, -0.27606606],\n       [-0.20602977, -0.08940771],\n       [ 0.20729873, -0.08199587],\n       [ 0.26309648,  0.36739483],\n       [ 0.13651058, -0.35549104],\n       [-0.29195088,  0.10620824],\n       [ 0.03803366, -0.01575941],\n       [-0.14032584, -0.23998621],\n       [ 0.15245304, -0.22797431],\n       [-0.1606122 , -0.01687026],\n       [-0.1956373 ,  0.19712535],\n       [-0.1534813 , -0.21791126],\n       [ 0.22740343,  0.06818458],\n       [ 0.16559258,  0.1989766 ]], dtype=float32)>), (None, <tf.Variable 'dense_146/bias:0' shape=(2,) dtype=float32, numpy=array([0., 0.], dtype=float32)>)).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ericp\\OneDrive\\Desktop\\Visual Studio Code Projekte\\Python Universität\\SOC_with_random_terminal_time\\OpenAI.ipynb Zelle 3\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ericp/OneDrive/Desktop/Visual%20Studio%20Code%20Projekte/Python%20Universit%C3%A4t/SOC_with_random_terminal_time/OpenAI.ipynb#W2sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m episodic_reward \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m reward\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ericp/OneDrive/Desktop/Visual%20Studio%20Code%20Projekte/Python%20Universit%C3%A4t/SOC_with_random_terminal_time/OpenAI.ipynb#W2sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m MC\u001b[39m.\u001b[39mbuffer\u001b[39m.\u001b[39mrecord((state,action,reward, new_state, done))\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/ericp/OneDrive/Desktop/Visual%20Studio%20Code%20Projekte/Python%20Universit%C3%A4t/SOC_with_random_terminal_time/OpenAI.ipynb#W2sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m MC\u001b[39m.\u001b[39;49mlearn()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ericp/OneDrive/Desktop/Visual%20Studio%20Code%20Projekte/Python%20Universit%C3%A4t/SOC_with_random_terminal_time/OpenAI.ipynb#W2sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m MC\u001b[39m.\u001b[39mupdate_target(MC\u001b[39m.\u001b[39mtarget_critic\u001b[39m.\u001b[39mvariables, MC\u001b[39m.\u001b[39mcritic\u001b[39m.\u001b[39mvariables)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ericp/OneDrive/Desktop/Visual%20Studio%20Code%20Projekte/Python%20Universit%C3%A4t/SOC_with_random_terminal_time/OpenAI.ipynb#W2sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m state \u001b[39m=\u001b[39m new_state\n",
      "\u001b[1;32mc:\\Users\\ericp\\OneDrive\\Desktop\\Visual Studio Code Projekte\\Python Universität\\SOC_with_random_terminal_time\\OpenAI.ipynb Zelle 3\u001b[0m in \u001b[0;36mMountainCar.learn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/ericp/OneDrive/Desktop/Visual%20Studio%20Code%20Projekte/Python%20Universit%C3%A4t/SOC_with_random_terminal_time/OpenAI.ipynb#W2sZmlsZQ%3D%3D?line=101'>102</a>\u001b[0m next_state_batch \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mconvert_to_tensor(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuffer\u001b[39m.\u001b[39mnext_state_buffer[batch_indices])\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/ericp/OneDrive/Desktop/Visual%20Studio%20Code%20Projekte/Python%20Universit%C3%A4t/SOC_with_random_terminal_time/OpenAI.ipynb#W2sZmlsZQ%3D%3D?line=102'>103</a>\u001b[0m done_batch \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mconvert_to_tensor(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuffer\u001b[39m.\u001b[39mdone_buffer[batch_indices])\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/ericp/OneDrive/Desktop/Visual%20Studio%20Code%20Projekte/Python%20Universit%C3%A4t/SOC_with_random_terminal_time/OpenAI.ipynb#W2sZmlsZQ%3D%3D?line=104'>105</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mupdate(state_batch, action_batch, reward_batch, next_state_batch,done_batch)\n",
      "\u001b[1;32mc:\\Users\\ericp\\OneDrive\\Desktop\\Visual Studio Code Projekte\\Python Universität\\SOC_with_random_terminal_time\\OpenAI.ipynb Zelle 3\u001b[0m in \u001b[0;36mMountainCar.update\u001b[1;34m(self, state_batch, action_batch, reward_batch, next_state_batch, done_batch)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ericp/OneDrive/Desktop/Visual%20Studio%20Code%20Projekte/Python%20Universit%C3%A4t/SOC_with_random_terminal_time/OpenAI.ipynb#W2sZmlsZQ%3D%3D?line=84'>85</a>\u001b[0m     critic_loss \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mreduce_mean(tf\u001b[39m.\u001b[39mmath\u001b[39m.\u001b[39msquare(dif))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ericp/OneDrive/Desktop/Visual%20Studio%20Code%20Projekte/Python%20Universit%C3%A4t/SOC_with_random_terminal_time/OpenAI.ipynb#W2sZmlsZQ%3D%3D?line=87'>88</a>\u001b[0m critic_grad \u001b[39m=\u001b[39m tape\u001b[39m.\u001b[39mgradient(critic_loss, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcritic\u001b[39m.\u001b[39mtrainable_variables)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/ericp/OneDrive/Desktop/Visual%20Studio%20Code%20Projekte/Python%20Universit%C3%A4t/SOC_with_random_terminal_time/OpenAI.ipynb#W2sZmlsZQ%3D%3D?line=88'>89</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcritic_optimizer\u001b[39m.\u001b[39;49mapply_gradients(\u001b[39mzip\u001b[39;49m(critic_grad, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcritic\u001b[39m.\u001b[39;49mtrainable_variables))\n",
      "File \u001b[1;32mc:\\Users\\ericp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\optimizer_v2\\optimizer_v2.py:633\u001b[0m, in \u001b[0;36mOptimizerV2.apply_gradients\u001b[1;34m(self, grads_and_vars, name, experimental_aggregate_gradients)\u001b[0m\n\u001b[0;32m    592\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_gradients\u001b[39m(\u001b[39mself\u001b[39m,\n\u001b[0;32m    593\u001b[0m                     grads_and_vars,\n\u001b[0;32m    594\u001b[0m                     name\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    595\u001b[0m                     experimental_aggregate_gradients\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m    596\u001b[0m   \u001b[39m\"\"\"Apply gradients to variables.\u001b[39;00m\n\u001b[0;32m    597\u001b[0m \n\u001b[0;32m    598\u001b[0m \u001b[39m  This is the second part of `minimize()`. It returns an `Operation` that\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    631\u001b[0m \u001b[39m    RuntimeError: If called in a cross-replica context.\u001b[39;00m\n\u001b[0;32m    632\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 633\u001b[0m   grads_and_vars \u001b[39m=\u001b[39m optimizer_utils\u001b[39m.\u001b[39;49mfilter_empty_gradients(grads_and_vars)\n\u001b[0;32m    634\u001b[0m   var_list \u001b[39m=\u001b[39m [v \u001b[39mfor\u001b[39;00m (_, v) \u001b[39min\u001b[39;00m grads_and_vars]\n\u001b[0;32m    636\u001b[0m   \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mname_scope(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_name):\n\u001b[0;32m    637\u001b[0m     \u001b[39m# Create iteration if necessary.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ericp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\optimizer_v2\\utils.py:73\u001b[0m, in \u001b[0;36mfilter_empty_gradients\u001b[1;34m(grads_and_vars)\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m filtered:\n\u001b[0;32m     72\u001b[0m   variable \u001b[39m=\u001b[39m ([v\u001b[39m.\u001b[39mname \u001b[39mfor\u001b[39;00m _, v \u001b[39min\u001b[39;00m grads_and_vars],)\n\u001b[1;32m---> 73\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNo gradients provided for any variable: \u001b[39m\u001b[39m{\u001b[39;00mvariable\u001b[39m}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     74\u001b[0m                    \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mProvided `grads_and_vars` is \u001b[39m\u001b[39m{\u001b[39;00mgrads_and_vars\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     75\u001b[0m \u001b[39mif\u001b[39;00m vars_with_empty_grads:\n\u001b[0;32m     76\u001b[0m   logging\u001b[39m.\u001b[39mwarning(\n\u001b[0;32m     77\u001b[0m       (\u001b[39m\"\u001b[39m\u001b[39mGradients do not exist for variables \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m when minimizing the loss. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     78\u001b[0m        \u001b[39m\"\u001b[39m\u001b[39mIf you\u001b[39m\u001b[39m'\u001b[39m\u001b[39mre using `model.compile()`, did you forget to provide a `loss`\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     79\u001b[0m        \u001b[39m\"\u001b[39m\u001b[39margument?\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[0;32m     80\u001b[0m       ([v\u001b[39m.\u001b[39mname \u001b[39mfor\u001b[39;00m v \u001b[39min\u001b[39;00m vars_with_empty_grads]))\n",
      "\u001b[1;31mValueError\u001b[0m: No gradients provided for any variable: (['dense_144/kernel:0', 'dense_144/bias:0', 'batch_normalization_48/gamma:0', 'batch_normalization_48/beta:0', 'dense_145/kernel:0', 'dense_145/bias:0', 'dense_146/kernel:0', 'dense_146/bias:0'],). Provided `grads_and_vars` is ((None, <tf.Variable 'dense_144/kernel:0' shape=(4, 32) dtype=float32, numpy=\narray([[-4.00936395e-01, -2.73486882e-01,  1.22568667e-01,\n         6.96156919e-02, -9.44505632e-02,  4.23667431e-02,\n        -1.74931169e-01,  2.66924918e-01,  3.07104945e-01,\n        -3.51616383e-01,  1.83020175e-01,  2.73801386e-01,\n         3.20230424e-01, -2.18223467e-01,  2.97599316e-01,\n        -4.06813115e-01, -1.49141848e-02, -2.08690673e-01,\n        -2.18872100e-01,  2.55886197e-01,  3.06799531e-01,\n         2.89450407e-01, -3.70396882e-01, -1.94990143e-01,\n         1.26841724e-01,  1.57133043e-01, -4.00922060e-01,\n        -2.48249635e-01,  3.04789484e-01,  4.02402043e-01,\n        -3.99610877e-01,  1.71460986e-01],\n       [-2.66813934e-01,  3.51128161e-01,  2.45302677e-01,\n         2.66947091e-01, -2.34754965e-01, -2.01329798e-01,\n        -2.44265169e-01,  2.25244284e-01,  3.09244931e-01,\n         2.17825055e-01,  1.72226548e-01, -3.07151884e-01,\n        -2.22975209e-01,  1.00005925e-01, -3.86716276e-01,\n        -8.73490572e-02,  4.01823103e-01, -2.74759322e-01,\n         1.46090448e-01,  1.93512201e-01, -4.81333137e-02,\n         8.66463184e-02, -9.58945155e-02, -1.44664079e-01,\n        -3.33165556e-01,  1.54869556e-01, -2.29780897e-01,\n         3.05997729e-02, -7.59783089e-02, -3.94917250e-01,\n        -1.47163868e-04,  1.72646999e-01],\n       [-3.97770017e-01,  2.12070704e-01,  3.85072708e-01,\n        -2.50706851e-01, -1.39537305e-01,  1.48036003e-01,\n        -9.17261839e-02,  4.97787297e-02, -2.68852800e-01,\n        -9.83342826e-02, -6.56511784e-02, -1.63328856e-01,\n        -1.84429884e-01, -2.51028419e-01, -8.40532482e-02,\n        -4.41760123e-02,  1.04923487e-01, -2.40390599e-02,\n         2.48792291e-01, -1.93467438e-01, -2.64415443e-01,\n        -1.35006607e-01, -1.25621974e-01, -4.03649479e-01,\n        -2.01423332e-01, -9.32657123e-02, -2.92053044e-01,\n         7.21679032e-02,  1.28977954e-01, -1.46484435e-01,\n        -8.31231475e-03, -3.72183621e-02],\n       [ 1.68977678e-02,  1.43736303e-01,  2.65276730e-01,\n        -3.69222045e-01,  2.20938981e-01, -3.70237529e-01,\n        -1.32144302e-01, -2.54301488e-01, -3.36276054e-01,\n         6.96541369e-02, -1.19539082e-01, -2.78853476e-01,\n         1.85334206e-01,  3.87022197e-01,  1.11860394e-01,\n        -1.34776711e-01,  5.81405163e-02,  1.68263674e-01,\n        -1.52286708e-01,  2.69138217e-01,  2.48173714e-01,\n        -7.60022700e-02, -1.23761803e-01,  5.62369525e-02,\n        -1.13178581e-01, -2.34067976e-01,  2.97794282e-01,\n        -1.52031302e-01, -3.35489303e-01, -1.84138268e-01,\n        -6.88560009e-02, -3.40509802e-01]], dtype=float32)>), (None, <tf.Variable 'dense_144/bias:0' shape=(32,) dtype=float32, numpy=\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n      dtype=float32)>), (None, <tf.Variable 'batch_normalization_48/gamma:0' shape=(32,) dtype=float32, numpy=\narray([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n      dtype=float32)>), (None, <tf.Variable 'batch_normalization_48/beta:0' shape=(32,) dtype=float32, numpy=\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n      dtype=float32)>), (None, <tf.Variable 'dense_145/kernel:0' shape=(32, 32) dtype=float32, numpy=\narray([[ 0.27584746, -0.06496839, -0.09250213, ...,  0.0032478 ,\n         0.09209427,  0.05609125],\n       [ 0.26899102, -0.18551037,  0.27240524, ...,  0.0933547 ,\n         0.1751354 , -0.06559962],\n       [ 0.25935897,  0.06988373,  0.01142356, ..., -0.2896401 ,\n         0.10134986, -0.04356027],\n       ...,\n       [-0.1810588 , -0.24481012,  0.10682935, ..., -0.19621429,\n         0.28625175, -0.2935816 ],\n       [ 0.2693306 , -0.11832605,  0.09773582, ..., -0.26795778,\n        -0.15686703,  0.29578075],\n       [-0.16326559, -0.12623784, -0.0426068 , ...,  0.19628242,\n        -0.21311536,  0.29363546]], dtype=float32)>), (None, <tf.Variable 'dense_145/bias:0' shape=(32,) dtype=float32, numpy=\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n      dtype=float32)>), (None, <tf.Variable 'dense_146/kernel:0' shape=(32, 2) dtype=float32, numpy=\narray([[-0.07349417, -0.08894059],\n       [-0.3475104 , -0.09511098],\n       [ 0.27645335,  0.32201687],\n       [ 0.25151494, -0.35627246],\n       [-0.04199424, -0.33878264],\n       [-0.40793544, -0.3201866 ],\n       [ 0.3453938 ,  0.05230099],\n       [ 0.14364138,  0.11987033],\n       [ 0.2981361 ,  0.00301608],\n       [-0.18693806, -0.22724429],\n       [-0.01885152,  0.0340575 ],\n       [ 0.3465357 ,  0.00538707],\n       [ 0.34752986, -0.05299577],\n       [ 0.3433369 ,  0.26623127],\n       [ 0.2642022 , -0.09142986],\n       [ 0.37769786,  0.02710146],\n       [-0.24965996, -0.29034677],\n       [ 0.21406886, -0.04898313],\n       [ 0.24426469, -0.27606606],\n       [-0.20602977, -0.08940771],\n       [ 0.20729873, -0.08199587],\n       [ 0.26309648,  0.36739483],\n       [ 0.13651058, -0.35549104],\n       [-0.29195088,  0.10620824],\n       [ 0.03803366, -0.01575941],\n       [-0.14032584, -0.23998621],\n       [ 0.15245304, -0.22797431],\n       [-0.1606122 , -0.01687026],\n       [-0.1956373 ,  0.19712535],\n       [-0.1534813 , -0.21791126],\n       [ 0.22740343,  0.06818458],\n       [ 0.16559258,  0.1989766 ]], dtype=float32)>), (None, <tf.Variable 'dense_146/bias:0' shape=(2,) dtype=float32, numpy=array([0., 0.], dtype=float32)>))."
     ]
    }
   ],
   "source": [
    "MC = MountainCar()\n",
    "\n",
    "ep_reward_list = []\n",
    "# To store average reward history of last few episodes\n",
    "avg_reward_list = []\n",
    "num_episode = 3000\n",
    "\n",
    "for ep in range(num_episode):\n",
    "    \n",
    "    done = False\n",
    "    state = MC.env.reset()\n",
    "    #state = tf.expand_dims(tf.convert_to_tensor(state),0)\n",
    "    state = np.reshape(state, [1,MC.state_dim])\n",
    "    episodic_reward = 0\n",
    "    t_counter = 0\n",
    "\n",
    "    if (ep % 100 == 0 and ep != 0):\n",
    "        MC.run_MC()\n",
    "    while(t_counter < 200):\n",
    "        MC.alpha = np.max([0.01,0.1 * (1 / (1 +ep))])\n",
    "        eps = np.max([0.01,0.9 * (100 / (100 +ep))])\n",
    "        action = MC.epsilon_greedy(state, eps)\n",
    "        \n",
    "        new_state, reward, done, info = MC.env.step(action)\n",
    "        \n",
    "        #new_state = tf.expand_dims(tf.convert_to_tensor(new_state.reshape(MC.state_dim)),0)\n",
    "        new_state = np.reshape(new_state, [1,MC.state_dim])\n",
    "        episodic_reward += reward\n",
    "        \n",
    "        MC.buffer.record((state,action,reward, new_state, done))\n",
    "       \n",
    "        MC.learn()\n",
    "    \n",
    "        MC.update_target(MC.target_critic.variables, MC.critic.variables)\n",
    "\n",
    "        state = new_state\n",
    "        t_counter +=1\n",
    "        if (done):\n",
    "            break\n",
    "    ep_reward_list.append(episodic_reward)\n",
    "    # Mean of last 40 episodes\n",
    "    avg_reward = np.mean(ep_reward_list[-20:])\n",
    "    print(\"Episode * {} * AVG Reward is ==> {}\".format(ep, avg_reward))\n",
    "    avg_reward_list.append(avg_reward)\n",
    "# Plotting graph\n",
    "# Episodes versus Avg. Rewards\n",
    "plt.plot(avg_reward_list)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Avg. Epsiodic Reward\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import gym # for environment\n",
    "from collections import deque\n",
    "from keras.layers import Dense\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ericp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gym\\envs\\registration.py:505: UserWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1` with the environment ID `CartPole-v1`.\u001b[0m\n",
      "  logger.warn(\n",
      "c:\\Users\\ericp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\optimizer_v2\\adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0, time: 11\n",
      "episode: 1, time: 15\n",
      "episode: 2, time: 25\n",
      "episode: 3, time: 10\n",
      "episode: 4, time: 40\n",
      "episode: 5, time: 46\n",
      "episode: 6, time: 21\n",
      "episode: 7, time: 29\n",
      "episode: 8, time: 20\n",
      "episode: 9, time: 12\n",
      "episode: 10, time: 18\n",
      "episode: 11, time: 36\n",
      "episode: 12, time: 68\n",
      "episode: 13, time: 63\n",
      "episode: 14, time: 49\n",
      "episode: 15, time: 39\n",
      "episode: 16, time: 24\n",
      "episode: 17, time: 24\n",
      "episode: 18, time: 49\n",
      "episode: 19, time: 59\n",
      "episode: 20, time: 30\n",
      "episode: 21, time: 62\n",
      "episode: 22, time: 32\n",
      "episode: 23, time: 65\n",
      "episode: 24, time: 67\n",
      "episode: 25, time: 65\n",
      "episode: 26, time: 63\n",
      "episode: 27, time: 36\n",
      "episode: 28, time: 56\n",
      "episode: 29, time: 66\n",
      "episode: 30, time: 47\n",
      "episode: 31, time: 54\n",
      "episode: 32, time: 55\n",
      "episode: 33, time: 121\n",
      "episode: 34, time: 60\n",
      "episode: 35, time: 94\n",
      "episode: 36, time: 75\n",
      "episode: 37, time: 146\n",
      "episode: 38, time: 114\n",
      "episode: 39, time: 123\n",
      "episode: 40, time: 200\n",
      "episode: 41, time: 200\n",
      "episode: 42, time: 200\n",
      "episode: 43, time: 200\n",
      "episode: 44, time: 184\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ericp\\OneDrive\\Desktop\\Visual Studio Code Projekte\\Python Universität\\SOC_with_random_terminal_time\\OpenAI.ipynb Zelle 5\u001b[0m in \u001b[0;36m<cell line: 68>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ericp/OneDrive/Desktop/Visual%20Studio%20Code%20Projekte/Python%20Universit%C3%A4t/SOC_with_random_terminal_time/OpenAI.ipynb#W4sZmlsZQ%3D%3D?line=95'>96</a>\u001b[0m state \u001b[39m=\u001b[39m next_state\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ericp/OneDrive/Desktop/Visual%20Studio%20Code%20Projekte/Python%20Universit%C3%A4t/SOC_with_random_terminal_time/OpenAI.ipynb#W4sZmlsZQ%3D%3D?line=97'>98</a>\u001b[0m \u001b[39m# replay\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/ericp/OneDrive/Desktop/Visual%20Studio%20Code%20Projekte/Python%20Universit%C3%A4t/SOC_with_random_terminal_time/OpenAI.ipynb#W4sZmlsZQ%3D%3D?line=98'>99</a>\u001b[0m agent\u001b[39m.\u001b[39;49mreplay(batch_size)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/ericp/OneDrive/Desktop/Visual%20Studio%20Code%20Projekte/Python%20Universit%C3%A4t/SOC_with_random_terminal_time/OpenAI.ipynb#W4sZmlsZQ%3D%3D?line=100'>101</a>\u001b[0m \u001b[39m# adjust epsilon\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/ericp/OneDrive/Desktop/Visual%20Studio%20Code%20Projekte/Python%20Universit%C3%A4t/SOC_with_random_terminal_time/OpenAI.ipynb#W4sZmlsZQ%3D%3D?line=101'>102</a>\u001b[0m agent\u001b[39m.\u001b[39madaptiveEGreedy()\n",
      "\u001b[1;32mc:\\Users\\ericp\\OneDrive\\Desktop\\Visual Studio Code Projekte\\Python Universität\\SOC_with_random_terminal_time\\OpenAI.ipynb Zelle 5\u001b[0m in \u001b[0;36mDQLAgent.replay\u001b[1;34m(self, batch_size)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ericp/OneDrive/Desktop/Visual%20Studio%20Code%20Projekte/Python%20Universit%C3%A4t/SOC_with_random_terminal_time/OpenAI.ipynb#W4sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m     target \u001b[39m=\u001b[39m reward \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgamma \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39mamax(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mpredict(next_state)[\u001b[39m0\u001b[39m]) \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ericp/OneDrive/Desktop/Visual%20Studio%20Code%20Projekte/Python%20Universit%C3%A4t/SOC_with_random_terminal_time/OpenAI.ipynb#W4sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m     \u001b[39m# target = R(s,a) + gamma * max Q`(s`,a`)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ericp/OneDrive/Desktop/Visual%20Studio%20Code%20Projekte/Python%20Universit%C3%A4t/SOC_with_random_terminal_time/OpenAI.ipynb#W4sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m     \u001b[39m# target (max Q` value) is output of Neural Network which takes s` as an input \u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ericp/OneDrive/Desktop/Visual%20Studio%20Code%20Projekte/Python%20Universit%C3%A4t/SOC_with_random_terminal_time/OpenAI.ipynb#W4sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m     \u001b[39m# amax(): flatten the lists (make them 1 list) and take max value\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/ericp/OneDrive/Desktop/Visual%20Studio%20Code%20Projekte/Python%20Universit%C3%A4t/SOC_with_random_terminal_time/OpenAI.ipynb#W4sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m train_target \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mpredict(state) \u001b[39m# s --> NN --> Q(s,a)=train_target\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ericp/OneDrive/Desktop/Visual%20Studio%20Code%20Projekte/Python%20Universit%C3%A4t/SOC_with_random_terminal_time/OpenAI.ipynb#W4sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m train_target[\u001b[39m0\u001b[39m][action] \u001b[39m=\u001b[39m target\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ericp/OneDrive/Desktop/Visual%20Studio%20Code%20Projekte/Python%20Universit%C3%A4t/SOC_with_random_terminal_time/OpenAI.ipynb#W4sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mfit(state, train_target, verbose \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ericp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 64\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\ericp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py:1978\u001b[0m, in \u001b[0;36mModel.predict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1976\u001b[0m callbacks\u001b[39m.\u001b[39mon_predict_begin()\n\u001b[0;32m   1977\u001b[0m batch_outputs \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1978\u001b[0m \u001b[39mfor\u001b[39;00m _, iterator \u001b[39min\u001b[39;00m data_handler\u001b[39m.\u001b[39menumerate_epochs():  \u001b[39m# Single epoch.\u001b[39;00m\n\u001b[0;32m   1979\u001b[0m   \u001b[39mwith\u001b[39;00m data_handler\u001b[39m.\u001b[39mcatch_stop_iteration():\n\u001b[0;32m   1980\u001b[0m     \u001b[39mfor\u001b[39;00m step \u001b[39min\u001b[39;00m data_handler\u001b[39m.\u001b[39msteps():\n",
      "File \u001b[1;32mc:\\Users\\ericp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\data_adapter.py:1191\u001b[0m, in \u001b[0;36mDataHandler.enumerate_epochs\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1189\u001b[0m \u001b[39m\"\"\"Yields `(epoch, tf.data.Iterator)`.\"\"\"\u001b[39;00m\n\u001b[0;32m   1190\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_truncate_execution_to_epoch():\n\u001b[1;32m-> 1191\u001b[0m   data_iterator \u001b[39m=\u001b[39m \u001b[39miter\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset)\n\u001b[0;32m   1192\u001b[0m   \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_initial_epoch, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_epochs):\n\u001b[0;32m   1193\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_insufficient_data:  \u001b[39m# Set by `catch_stop_iteration`.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ericp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:486\u001b[0m, in \u001b[0;36mDatasetV2.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    484\u001b[0m \u001b[39mif\u001b[39;00m context\u001b[39m.\u001b[39mexecuting_eagerly() \u001b[39mor\u001b[39;00m ops\u001b[39m.\u001b[39minside_function():\n\u001b[0;32m    485\u001b[0m   \u001b[39mwith\u001b[39;00m ops\u001b[39m.\u001b[39mcolocate_with(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variant_tensor):\n\u001b[1;32m--> 486\u001b[0m     \u001b[39mreturn\u001b[39;00m iterator_ops\u001b[39m.\u001b[39;49mOwnedIterator(\u001b[39mself\u001b[39;49m)\n\u001b[0;32m    487\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    488\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m`tf.data.Dataset` only supports Python-style \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    489\u001b[0m                      \u001b[39m\"\u001b[39m\u001b[39miteration in eager mode or within tf.function.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ericp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:755\u001b[0m, in \u001b[0;36mOwnedIterator.__init__\u001b[1;34m(self, dataset, components, element_spec)\u001b[0m\n\u001b[0;32m    751\u001b[0m   \u001b[39mif\u001b[39;00m (components \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m element_spec \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    752\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    753\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWhen `dataset` is provided, `element_spec` and `components` must \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    754\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mnot be specified.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 755\u001b[0m   \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_iterator(dataset)\n\u001b[0;32m    757\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_next_call_count \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\ericp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:787\u001b[0m, in \u001b[0;36mOwnedIterator._create_iterator\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    782\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    783\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterator_resource, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_deleter \u001b[39m=\u001b[39m (\n\u001b[0;32m    784\u001b[0m       gen_dataset_ops\u001b[39m.\u001b[39manonymous_iterator_v2(\n\u001b[0;32m    785\u001b[0m           output_types\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flat_output_types,\n\u001b[0;32m    786\u001b[0m           output_shapes\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flat_output_shapes))\n\u001b[1;32m--> 787\u001b[0m   gen_dataset_ops\u001b[39m.\u001b[39;49mmake_iterator(ds_variant, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_iterator_resource)\n\u001b[0;32m    788\u001b[0m   \u001b[39m# Delete the resource when this object is deleted\u001b[39;00m\n\u001b[0;32m    789\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_resource_deleter \u001b[39m=\u001b[39m IteratorResourceDeleter(\n\u001b[0;32m    790\u001b[0m       handle\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterator_resource,\n\u001b[0;32m    791\u001b[0m       deleter\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_deleter)\n",
      "File \u001b[1;32mc:\\Users\\ericp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py:3314\u001b[0m, in \u001b[0;36mmake_iterator\u001b[1;34m(dataset, iterator, name)\u001b[0m\n\u001b[0;32m   3312\u001b[0m \u001b[39mif\u001b[39;00m tld\u001b[39m.\u001b[39mis_eager:\n\u001b[0;32m   3313\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 3314\u001b[0m     _result \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_FastPathExecute(\n\u001b[0;32m   3315\u001b[0m       _ctx, \u001b[39m\"\u001b[39;49m\u001b[39mMakeIterator\u001b[39;49m\u001b[39m\"\u001b[39;49m, name, dataset, iterator)\n\u001b[0;32m   3316\u001b[0m     \u001b[39mreturn\u001b[39;00m _result\n\u001b[0;32m   3317\u001b[0m   \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class DQLAgent(): \n",
    "    \n",
    "    def __init__(self, env):\n",
    "        # parameters and hyperparameters\n",
    "        \n",
    "        # this part is for neural network or build_model()\n",
    "        self.state_size = env.observation_space.shape[0] # this is for input of neural network node size\n",
    "        self.action_size = env.action_space.n # this is for out of neural network node size\n",
    "        \n",
    "        # this part is for replay()\n",
    "        self.gamma = 0.95\n",
    "        self.learning_rate = 0.001\n",
    "        \n",
    "        # this part is for adaptiveEGreedy()\n",
    "        self.epsilon = 1 # initial exploration rate\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.epsilon_min = 0.01\n",
    "        \n",
    "        self.memory = deque(maxlen = 1000) # a list with 1000 memory, if it becomes full first inputs will be deleted\n",
    "        \n",
    "        self.model = self.build_model()\n",
    "    \n",
    "    def build_model(self):\n",
    "        # neural network for deep Q learning\n",
    "        model = keras.Sequential()\n",
    "        model.add(Dense(48, input_dim = self.state_size, activation = 'tanh')) # first hidden layer\n",
    "        model.add(Dense(self.action_size, activation = 'linear')) # output layer\n",
    "        model.compile(loss = 'mse', optimizer = tf.keras.optimizers.Adam(lr = self.learning_rate))\n",
    "        return model\n",
    "    \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        # storage\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def act(self, state):\n",
    "        # acting, exploit or explore\n",
    "        if random.uniform(0,1) <= self.epsilon:\n",
    "            return env.action_space.sample()\n",
    "        else:\n",
    "            act_values = self.model.predict(state)\n",
    "            return np.argmax(act_values[0])\n",
    "            \n",
    "    \n",
    "    def replay(self, batch_size):\n",
    "        # training\n",
    "        \n",
    "        if len(self.memory) < batch_size:\n",
    "            return # memory is still not full\n",
    "        \n",
    "        minibatch = random.sample(self.memory, batch_size) # take 16 (batch_size) random samples from memory\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            if done: # if the game is over, I dont have next state, I just have reward \n",
    "                target = reward\n",
    "            else:\n",
    "                target = reward + self.gamma * np.amax(self.model.predict(next_state)[0]) \n",
    "                # target = R(s,a) + gamma * max Q`(s`,a`)\n",
    "                # target (max Q` value) is output of Neural Network which takes s` as an input \n",
    "                # amax(): flatten the lists (make them 1 list) and take max value\n",
    "            train_target = self.model.predict(state) # s --> NN --> Q(s,a)=train_target\n",
    "            train_target[0][action] = target\n",
    "            self.model.fit(state, train_target, verbose = 0) # verbose: dont show loss and epoch\n",
    "    \n",
    "    def adaptiveEGreedy(self):\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "            \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # initialize gym environment and agent\n",
    "    env = gym.make('CartPole-v0')\n",
    "    agent = DQLAgent(env)\n",
    "\n",
    "    batch_size = 16\n",
    "    episodes = 50\n",
    "    for e in range(episodes):\n",
    "        \n",
    "        # initialize environment\n",
    "        state = env.reset()\n",
    "        state = np.reshape(state, [1,4])\n",
    "        \n",
    "        time = 0 # each second I will get reward, because I want to sustain a balance forever\n",
    "        while True:\n",
    "            \n",
    "            # act\n",
    "            action = agent.act(state)\n",
    "            \n",
    "            # step\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            next_state = np.reshape(next_state, [1,4])\n",
    "            \n",
    "            # remember / storage\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            \n",
    "            # update state\n",
    "            state = next_state\n",
    "            \n",
    "            # replay\n",
    "            agent.replay(batch_size)\n",
    "            \n",
    "            # adjust epsilon\n",
    "            agent.adaptiveEGreedy()\n",
    "            \n",
    "            time += 1\n",
    "            \n",
    "            if done:\n",
    "                print('episode: {}, time: {}'.format(e, time))\n",
    "                break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369b169b6a5bc372c49943446f64d019decaea6189ec6cd1dc9e297757cbbf29"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
