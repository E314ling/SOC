{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import gym\n",
    "import pygame\n",
    "from keras import layers, optimizers, losses\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class experience_memory():\n",
    "\n",
    "    def __init__(self, buffer_capacity, batch_size, state_dim, action_dim):\n",
    "        # Number of \"experiences\" to store at max\n",
    "        self.buffer_capacity = buffer_capacity\n",
    "        # Num of tuples to train on.\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # Its tells us num of times record() was called.\n",
    "        self.buffer_counter = 0\n",
    "\n",
    "        # Instead of list of tuples as the exp.replay concept go\n",
    "        # We use different np.arrays for each tuple element\n",
    "        self.state_buffer = np.zeros((self.buffer_capacity, state_dim), dtype=np.float32)\n",
    "        self.action_buffer = np.zeros((self.buffer_capacity, action_dim), dtype=np.int32)\n",
    "        self.reward_buffer = np.zeros((self.buffer_capacity, 1), dtype=np.float32)\n",
    "        self.next_state_buffer = np.zeros((self.buffer_capacity, state_dim), dtype=np.float32)\n",
    "        self.done_buffer = np.zeros((self.buffer_capacity, 1), dtype=np.float32)\n",
    "\n",
    "    # Takes (s,a,r,s') obervation tuple as input\n",
    "    def record(self, obs_tuple):\n",
    "        # Set index to zero if buffer_capacity is exceeded,\n",
    "        # replacing old records\n",
    "        index = self.buffer_counter % self.buffer_capacity\n",
    "\n",
    "        self.state_buffer[index] = obs_tuple[0]\n",
    "        self.action_buffer[index] = obs_tuple[1]\n",
    "        self.reward_buffer[index] = obs_tuple[2]\n",
    "        self.next_state_buffer[index] = obs_tuple[3]\n",
    "        self.done_buffer[index] = obs_tuple[4]\n",
    "\n",
    "\n",
    "        self.buffer_counter += 1\n",
    "\n",
    "class MountainCar():\n",
    "\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "        self.batch_size = 32\n",
    "        self.max_memory_size = 500000\n",
    "\n",
    "        self.state_dim = 4\n",
    "        self.action_dim = 1\n",
    "\n",
    "        self.gamma = 0.99\n",
    "        self.tau = 0.001\n",
    "        self.lower_action_bound = -1\n",
    "        self.upper_action_bound = 1\n",
    "\n",
    "        self.action_space = np.array([0,1])\n",
    "        self.num_a = len(self.action_space)\n",
    "\n",
    "        self.buffer = experience_memory(self.max_memory_size, self.batch_size, self.state_dim, self.action_dim)\n",
    "\n",
    "        # init the neural netsf\n",
    "        self.critic = self.get_critic_NN()\n",
    "        self.target_critic = self.get_critic_NN()\n",
    "        self.target_critic.set_weights(self.critic.get_weights())\n",
    "        self.eps = 1\n",
    "        self.alpha = 0.001\n",
    "        self.critic_optimizer = tf.keras.optimizers.Adam(self.alpha)\n",
    "\n",
    "    @tf.function\n",
    "    def update(self, state_batch, action_batch, reward_batch, next_state_batch, done_batch):\n",
    "             \n",
    "        \n",
    "        next_q_vals =  self.target_critic(next_state_batch)\n",
    "            \n",
    "        target_vals = tf.reshape(tf.reduce_max(next_q_vals, axis =1),[self.batch_size,1])\n",
    "        y = reward_batch + (1-done_batch)* self.gamma*target_vals\n",
    "        y = tf.reshape(y,[self.batch_size,])\n",
    "        mask = tf.reshape(tf.one_hot(action_batch, self.num_a, dtype=tf.float32),[self.batch_size, self.num_a])\n",
    "        with tf.GradientTape() as tape:\n",
    "            \n",
    "            critic_value = self.critic(state_batch)\n",
    "      \n",
    "            critic_pred = tf.reduce_sum(tf.multiply(critic_value,mask), axis=1)\n",
    "          \n",
    "            #critic_loss = tf.reduce_mean(tf.math.square(y-critic_value))\n",
    "            critic_loss = losses.MSE(y,critic_pred)\n",
    "        \n",
    "        \n",
    "        critic_grad = tape.gradient(critic_loss, self.critic.trainable_variables)\n",
    "        critic_grad, _ = tf.clip_by_global_norm(critic_grad, 2.0)\n",
    "        self.critic_optimizer.apply_gradients(zip(critic_grad, self.critic.trainable_variables))\n",
    "\n",
    "        \n",
    "    def learn(self):\n",
    "        # get sample\n",
    "\n",
    "        record_range = min(self.buffer.buffer_counter, self.buffer.buffer_capacity)\n",
    "\n",
    "        batch_indices = np.random.choice(record_range, self.batch_size)\n",
    "        \n",
    "        state_batch = tf.convert_to_tensor(self.buffer.state_buffer[batch_indices])\n",
    "        action_batch = tf.convert_to_tensor(self.buffer.action_buffer[batch_indices])\n",
    "        reward_batch = tf.convert_to_tensor(self.buffer.reward_buffer[batch_indices])\n",
    "        next_state_batch = tf.convert_to_tensor(self.buffer.next_state_buffer[batch_indices])\n",
    "        done_batch = tf.convert_to_tensor(self.buffer.done_buffer[batch_indices])\n",
    "        \n",
    "        self.update(state_batch, action_batch, reward_batch, next_state_batch,done_batch)\n",
    "\n",
    "    @tf.function\n",
    "    def update_target(self, target_weights, weights):\n",
    "        for (a,b) in zip(target_weights, weights):\n",
    "            a.assign(self.tau *b + (1-self.tau) *a)\n",
    "\n",
    "    def get_critic_NN(self):\n",
    "        # input [state, action]\n",
    "        \n",
    "        state_input = layers.Input(shape =(self.state_dim,))\n",
    "\n",
    "        out = layers.Dense(32, activation = 'relu')(state_input)\n",
    "        out = layers.BatchNormalization()(out)\n",
    "        out = layers.Dense(32, activation = 'relu')(out)\n",
    "        out = layers.Dense(self.num_a)(out)\n",
    "\n",
    "        \n",
    "        model = keras.Model(inputs = state_input, outputs = out)\n",
    "        model.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(learning_rate= 0.001),\n",
    "            loss=tf.keras.losses.MeanSquaredError(),\n",
    "            metrics=[tf.keras.metrics.MeanSquaredError()],\n",
    "        )\n",
    "        return model\n",
    "\n",
    "    def epsilon_greedy(self, state, eps):\n",
    "\n",
    "        q_vals = self.critic(state)\n",
    "        \n",
    "        if (eps > np.random.rand()):\n",
    "           \n",
    "            rand_ind = np.random.choice(self.num_a)\n",
    "            \n",
    "            return self.action_space[rand_ind]\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            a_ind = tf.argmax(q_vals,axis = 1)[0]\n",
    "            \n",
    "            return self.action_space[a_ind]\n",
    "    \n",
    "    def run_MC(self):\n",
    "        done = False\n",
    "        state = self.env.reset()\n",
    "        state = tf.expand_dims(tf.convert_to_tensor(state),0)\n",
    "        t_counter = 0\n",
    "        while (True):\n",
    "            self.env.render()\n",
    "            time.sleep(0.01)\n",
    "            \n",
    "            a_ind = np.argmax(self.critic(state))\n",
    "            \n",
    "            action = self.action_space[a_ind]\n",
    "            new_state, reward, done, info = self.env.step(action)\n",
    "        \n",
    "            new_state = tf.expand_dims(tf.convert_to_tensor(new_state.reshape(self.state_dim)),0)\n",
    "            state = new_state\n",
    "            t_counter += 1\n",
    "\n",
    "            if (done):\n",
    "                break\n",
    "        print('Zeit: ', t_counter)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([ 0.03865113, -0.0289197 ,  0.0269503 , -0.02833511], dtype=float32), {})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Users\\pillieri\\Desktop\\Visual Studio Code Projekte\\Python Projekte\\.venv\\lib\\site-packages\\numpy\\core\\fromnumeric.py:43: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  result = getattr(asarray(obj), method)(*args, **kwds)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 2 into shape (1,4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [3], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[39m#state = tf.expand_dims(tf.convert_to_tensor(state),0)\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[39mprint\u001b[39m(state)\n\u001b[1;32m---> 14\u001b[0m state \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mreshape(state, [\u001b[39m1\u001b[39;49m,MC\u001b[39m.\u001b[39;49mstate_dim])\n\u001b[0;32m     15\u001b[0m episodic_reward \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m     16\u001b[0m t_counter \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mreshape\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32md:\\Users\\pillieri\\Desktop\\Visual Studio Code Projekte\\Python Projekte\\.venv\\lib\\site-packages\\numpy\\core\\fromnumeric.py:298\u001b[0m, in \u001b[0;36mreshape\u001b[1;34m(a, newshape, order)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[39m@array_function_dispatch\u001b[39m(_reshape_dispatcher)\n\u001b[0;32m    199\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mreshape\u001b[39m(a, newshape, order\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mC\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m    200\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    201\u001b[0m \u001b[39m    Gives a new shape to an array without changing its data.\u001b[39;00m\n\u001b[0;32m    202\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    296\u001b[0m \u001b[39m           [5, 6]])\u001b[39;00m\n\u001b[0;32m    297\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 298\u001b[0m     \u001b[39mreturn\u001b[39;00m _wrapfunc(a, \u001b[39m'\u001b[39;49m\u001b[39mreshape\u001b[39;49m\u001b[39m'\u001b[39;49m, newshape, order\u001b[39m=\u001b[39;49morder)\n",
      "File \u001b[1;32md:\\Users\\pillieri\\Desktop\\Visual Studio Code Projekte\\Python Projekte\\.venv\\lib\\site-packages\\numpy\\core\\fromnumeric.py:54\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     52\u001b[0m bound \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(obj, method, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m     53\u001b[0m \u001b[39mif\u001b[39;00m bound \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m---> 54\u001b[0m     \u001b[39mreturn\u001b[39;00m _wrapit(obj, method, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m     56\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     57\u001b[0m     \u001b[39mreturn\u001b[39;00m bound(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n",
      "File \u001b[1;32md:\\Users\\pillieri\\Desktop\\Visual Studio Code Projekte\\Python Projekte\\.venv\\lib\\site-packages\\numpy\\core\\fromnumeric.py:43\u001b[0m, in \u001b[0;36m_wrapit\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m:\n\u001b[0;32m     42\u001b[0m     wrap \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m---> 43\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(asarray(obj), method)(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m     44\u001b[0m \u001b[39mif\u001b[39;00m wrap:\n\u001b[0;32m     45\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(result, mu\u001b[39m.\u001b[39mndarray):\n",
      "\u001b[1;31mValueError\u001b[0m: cannot reshape array of size 2 into shape (1,4)"
     ]
    }
   ],
   "source": [
    "MC = MountainCar()\n",
    "\n",
    "ep_reward_list = []\n",
    "# To store average reward history of last few episodes\n",
    "avg_reward_list = []\n",
    "num_episode = 5000\n",
    "decay = 0.9999\n",
    "for ep in range(num_episode):\n",
    "    \n",
    "    done = False\n",
    "    state = MC.env.reset()\n",
    "    #state = tf.expand_dims(tf.convert_to_tensor(state),0)\n",
    "    \n",
    "    state = np.reshape(state, [1,MC.state_dim])\n",
    "    episodic_reward = 0\n",
    "    t_counter = 0\n",
    "\n",
    "    #if (ep % 100 == 0 and ep != 0):\n",
    "        #MC.run_MC()\n",
    "    while(t_counter < 200):\n",
    "        MC.alpha = 0.0001  #np.max([0.0001,0.01 * (1 / (1 +ep))])\n",
    "        MC.eps = np.max([0.01, MC.eps * decay])\n",
    "        action = MC.epsilon_greedy(state, MC.eps)\n",
    "        \n",
    "        new_state, reward, done, info = MC.env.step(action)\n",
    "        \n",
    "        #new_state = tf.expand_dims(tf.convert_to_tensor(new_state.reshape(MC.state_dim)),0)\n",
    "       \n",
    "        new_state = np.reshape(new_state, [1,MC.state_dim])\n",
    "        episodic_reward += reward\n",
    "        \n",
    "        MC.buffer.record((state,action,reward, new_state, done))\n",
    "       \n",
    "        MC.learn()\n",
    "    \n",
    "        MC.update_target(MC.target_critic.variables, MC.critic.variables)\n",
    "\n",
    "        state = new_state\n",
    "        t_counter +=1\n",
    "        if (done):\n",
    "            break\n",
    "    ep_reward_list.append(episodic_reward)\n",
    "    # Mean of last 40 episodes\n",
    "    avg_reward = np.mean(ep_reward_list[-20:])\n",
    "    print(\"Episode * {} * AVG Reward is ==> {}, eps ==> {}\".format(ep, avg_reward, MC.eps))\n",
    "    avg_reward_list.append(avg_reward)\n",
    "# Plotting graph\n",
    "# Episodes versus Avg. Rewards\n",
    "plt.plot(avg_reward_list)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Avg. Epsiodic Reward\")\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0 (tags/v3.10.0:b494f59, Oct  4 2021, 19:00:18) [MSC v.1929 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369b169b6a5bc372c49943446f64d019decaea6189ec6cd1dc9e297757cbbf29"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
