{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import gym\n",
    "from keras import layers, optimizers, losses\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class experience_memory():\n",
    "\n",
    "    def __init__(self, buffer_capacity, batch_size, state_dim, action_dim):\n",
    "        # Number of \"experiences\" to store at max\n",
    "        self.buffer_capacity = buffer_capacity\n",
    "        # Num of tuples to train on.\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # Its tells us num of times record() was called.\n",
    "        self.buffer_counter = 0\n",
    "\n",
    "        # Instead of list of tuples as the exp.replay concept go\n",
    "        # We use different np.arrays for each tuple element\n",
    "        self.state_buffer = np.zeros((self.buffer_capacity, state_dim), dtype=np.float32)\n",
    "        self.action_buffer = np.zeros((self.buffer_capacity, action_dim), dtype=np.float32)\n",
    "        self.reward_buffer = np.zeros((self.buffer_capacity, 1), dtype=np.float32)\n",
    "        self.next_state_buffer = np.zeros((self.buffer_capacity, state_dim), dtype=np.float32)\n",
    "        self.done_buffer = np.zeros((self.buffer_capacity, 1), dtype=np.float32)\n",
    "\n",
    "    # Takes (s,a,r,s') obervation tuple as input\n",
    "    def record(self, obs_tuple):\n",
    "        # Set index to zero if buffer_capacity is exceeded,\n",
    "        # replacing old records\n",
    "        index = self.buffer_counter % self.buffer_capacity\n",
    "\n",
    "        self.state_buffer[index] = obs_tuple[0]\n",
    "        self.action_buffer[index] = obs_tuple[1]\n",
    "        self.reward_buffer[index] = obs_tuple[2]\n",
    "        self.next_state_buffer[index] = obs_tuple[3]\n",
    "        self.done_buffer[index] = obs_tuple[4]\n",
    "\n",
    "\n",
    "        self.buffer_counter += 1\n",
    "\n",
    "class MountainCar():\n",
    "\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "        self.batch_size = 64\n",
    "        self.max_memory_size = 10000\n",
    "\n",
    "        self.state_dim = 4\n",
    "        self.action_dim = 1\n",
    "\n",
    "        self.gamma = 0.99\n",
    "        self.tau = 0.01\n",
    "        self.lower_action_bound = -1\n",
    "        self.upper_action_bound = 1\n",
    "\n",
    "        self.action_space = np.array([0,1])\n",
    "        self.num_a = len(self.action_space)\n",
    "\n",
    "        self.buffer = experience_memory(self.max_memory_size, self.batch_size, self.state_dim, self.action_dim)\n",
    "\n",
    "        # init the neural netsf\n",
    "        self.critic = self.get_critic_NN()\n",
    "        self.target_critic = self.get_critic_NN()\n",
    "        self.alpha = 1\n",
    "        self.critic_optimizer = tf.keras.optimizers.Adam(self.alpha)\n",
    "\n",
    "    #@tf.function\n",
    "    def update(self, state_batch, action_batch, reward_batch, next_state_batch, done_batch):\n",
    "             \n",
    "        next_q_vals =  self.target_critic(next_state_batch, training=True)\n",
    "        \n",
    "        target_vals = tf.reduce_max(next_q_vals, axis =1)\n",
    "        target_vals = tf.reshape(target_vals, [self.batch_size,1])\n",
    "        y = reward_batch + (tf.ones_like(done_batch,dtype = np.float32)-done_batch)* self.gamma*target_vals\n",
    "        \n",
    "    \n",
    "  \n",
    "     \n",
    "\n",
    "        \n",
    "        self.critic.fit(state_batch,\n",
    "        y = y,\n",
    "        verbose = 0,\n",
    "        batch_size =  self.batch_size)\n",
    "\n",
    "\n",
    "        # with tf.GradientTape() as tape:\n",
    "            \n",
    "        #     next_q_vals =  self.target_critic(next_state_batch, training=True)\n",
    "            \n",
    "        #     target_vals = tf.reduce_max(next_q_vals, axis =1)\n",
    "        #     y = reward_batch + (tf.ones_like(done_batch)-done_batch)* self.gamma*target_vals\n",
    "           \n",
    "        #     critic_value = tf.reduce_max(self.critic(state_batch, training=True),axis =1)\n",
    "            \n",
    "        #     critic_loss = tf.math.reduce_mean(tf.math.square(y- critic_value))\n",
    "        \n",
    "        \n",
    "        # critic_grad = tape.gradient(critic_loss, self.critic.trainable_variables)\n",
    "        # self.critic_optimizer.apply_gradients(zip(critic_grad, self.critic.trainable_variables))\n",
    "\n",
    "        \n",
    "    def learn(self):\n",
    "        # get sample\n",
    "\n",
    "        record_range = min(self.buffer.buffer_counter, self.buffer.buffer_capacity)\n",
    "\n",
    "        batch_indices = np.random.choice(record_range, self.batch_size)\n",
    "        \n",
    "        state_batch = tf.convert_to_tensor(self.buffer.state_buffer[batch_indices])\n",
    "        action_batch = tf.convert_to_tensor(self.buffer.action_buffer[batch_indices])\n",
    "        reward_batch = tf.convert_to_tensor(self.buffer.reward_buffer[batch_indices])\n",
    "        next_state_batch = tf.convert_to_tensor(self.buffer.next_state_buffer[batch_indices])\n",
    "        done_batch = tf.convert_to_tensor(self.buffer.done_buffer[batch_indices])\n",
    "        \n",
    "        self.update(state_batch, action_batch, reward_batch, next_state_batch,done_batch)\n",
    "\n",
    "    @tf.function\n",
    "    def update_target(self, target_weights, weights):\n",
    "        for (a,b) in zip(target_weights, weights):\n",
    "            a.assign(self.tau *b + (1-self.tau) *a)\n",
    "\n",
    "    def get_critic_NN(self):\n",
    "        # input [state, action]\n",
    "        \n",
    "        state_input = layers.Input(shape =(self.state_dim,))\n",
    "\n",
    "        out = layers.Dense(256, activation = 'relu')(state_input)\n",
    "        #out = layers.BatchNormalization()(out)\n",
    "        out = layers.Dense(256, activation = 'relu')(out)\n",
    "        out = layers.Dense(self.num_a)(out)\n",
    "\n",
    "        \n",
    "        model = keras.Model(inputs = state_input, outputs = out)\n",
    "        model.compile(\n",
    "            optimizer='adam',\n",
    "            loss=tf.keras.losses.MeanSquaredError(),\n",
    "            metrics=[tf.keras.metrics.MeanSquaredError()],\n",
    "        )\n",
    "        return model\n",
    "\n",
    "    def epsilon_greedy(self, state, eps):\n",
    "\n",
    "        q_vals = self.critic(state)\n",
    "        \n",
    "        if (eps > np.random.rand()):\n",
    "           \n",
    "            rand_ind = np.random.choice(self.num_a)\n",
    "            \n",
    "            return self.action_space[rand_ind]\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            a_ind = tf.argmax(q_vals,axis = 1)\n",
    "           \n",
    "            return self.action_space[a_ind]\n",
    "    \n",
    "    def run_MC(self):\n",
    "        done = False\n",
    "        state = self.env.reset()\n",
    "        state = tf.expand_dims(tf.convert_to_tensor(state),0)\n",
    "        t_counter = 0\n",
    "        while (True):\n",
    "            self.env.render()\n",
    "            time.sleep(0.01)\n",
    "            \n",
    "            a_ind = np.argmax(self.critic(state))\n",
    "            \n",
    "            action = self.action_space[a_ind]\n",
    "            new_state, reward, done, info = self.env.step(action)\n",
    "        \n",
    "            new_state = tf.expand_dims(tf.convert_to_tensor(new_state.reshape(self.state_dim)),0)\n",
    "            state = new_state\n",
    "            t_counter += 1\n",
    "\n",
    "            if (done):\n",
    "                break\n",
    "        print('Zeit: ', t_counter)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ericp\\AppData\\Local\\Temp\\ipykernel_34276\\1421666194.py:152: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return self.action_space[a_ind]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode * 0 * Reward is ==> 16.0\n",
      "Episode * 1 * Reward is ==> 18.0\n",
      "Episode * 2 * Reward is ==> 10.0\n",
      "Episode * 3 * Reward is ==> 28.0\n",
      "Episode * 4 * Reward is ==> 15.0\n",
      "Episode * 5 * Reward is ==> 11.0\n",
      "Episode * 6 * Reward is ==> 10.0\n",
      "Episode * 7 * Reward is ==> 13.0\n",
      "Episode * 8 * Reward is ==> 14.0\n",
      "Episode * 9 * Reward is ==> 10.0\n",
      "Episode * 10 * Reward is ==> 22.0\n",
      "Episode * 11 * Reward is ==> 11.0\n",
      "Episode * 12 * Reward is ==> 13.0\n",
      "Episode * 13 * Reward is ==> 10.0\n",
      "Episode * 14 * Reward is ==> 33.0\n",
      "Episode * 15 * Reward is ==> 15.0\n",
      "Episode * 16 * Reward is ==> 46.0\n",
      "Episode * 17 * Reward is ==> 26.0\n",
      "Episode * 18 * Reward is ==> 13.0\n",
      "Episode * 19 * Reward is ==> 15.0\n",
      "Episode * 20 * Reward is ==> 39.0\n",
      "Episode * 21 * Reward is ==> 18.0\n",
      "Episode * 22 * Reward is ==> 29.0\n",
      "Episode * 23 * Reward is ==> 21.0\n",
      "Episode * 24 * Reward is ==> 12.0\n",
      "Episode * 25 * Reward is ==> 13.0\n",
      "Episode * 26 * Reward is ==> 29.0\n",
      "Episode * 27 * Reward is ==> 16.0\n",
      "Episode * 28 * Reward is ==> 37.0\n",
      "Episode * 29 * Reward is ==> 13.0\n",
      "Episode * 30 * Reward is ==> 9.0\n",
      "Episode * 31 * Reward is ==> 12.0\n",
      "Episode * 32 * Reward is ==> 11.0\n",
      "Episode * 33 * Reward is ==> 17.0\n",
      "Episode * 34 * Reward is ==> 9.0\n",
      "Episode * 35 * Reward is ==> 10.0\n",
      "Episode * 36 * Reward is ==> 16.0\n",
      "Episode * 37 * Reward is ==> 41.0\n",
      "Episode * 38 * Reward is ==> 38.0\n",
      "Episode * 39 * Reward is ==> 9.0\n",
      "Episode * 40 * Reward is ==> 19.0\n",
      "Episode * 41 * Reward is ==> 17.0\n",
      "Episode * 42 * Reward is ==> 11.0\n",
      "Episode * 43 * Reward is ==> 17.0\n",
      "Episode * 44 * Reward is ==> 21.0\n",
      "Episode * 45 * Reward is ==> 20.0\n",
      "Episode * 46 * Reward is ==> 44.0\n",
      "Episode * 47 * Reward is ==> 15.0\n",
      "Episode * 48 * Reward is ==> 17.0\n",
      "Episode * 49 * Reward is ==> 13.0\n",
      "Episode * 50 * Reward is ==> 11.0\n",
      "Episode * 51 * Reward is ==> 16.0\n",
      "Episode * 52 * Reward is ==> 28.0\n",
      "Episode * 53 * Reward is ==> 10.0\n",
      "Episode * 54 * Reward is ==> 12.0\n",
      "Episode * 55 * Reward is ==> 23.0\n",
      "Episode * 56 * Reward is ==> 13.0\n",
      "Episode * 57 * Reward is ==> 11.0\n",
      "Episode * 58 * Reward is ==> 15.0\n",
      "Episode * 59 * Reward is ==> 13.0\n",
      "Episode * 60 * Reward is ==> 9.0\n",
      "Episode * 61 * Reward is ==> 25.0\n",
      "Episode * 62 * Reward is ==> 14.0\n",
      "Episode * 63 * Reward is ==> 15.0\n",
      "Episode * 64 * Reward is ==> 9.0\n",
      "Episode * 65 * Reward is ==> 27.0\n",
      "Episode * 66 * Reward is ==> 36.0\n",
      "Episode * 67 * Reward is ==> 25.0\n",
      "Episode * 68 * Reward is ==> 10.0\n",
      "Episode * 69 * Reward is ==> 15.0\n",
      "Episode * 70 * Reward is ==> 10.0\n",
      "Episode * 71 * Reward is ==> 30.0\n",
      "Episode * 72 * Reward is ==> 23.0\n",
      "Episode * 73 * Reward is ==> 21.0\n",
      "Episode * 74 * Reward is ==> 16.0\n",
      "Episode * 75 * Reward is ==> 37.0\n",
      "Episode * 76 * Reward is ==> 30.0\n",
      "Episode * 77 * Reward is ==> 30.0\n",
      "Episode * 78 * Reward is ==> 9.0\n",
      "Episode * 79 * Reward is ==> 16.0\n",
      "Episode * 80 * Reward is ==> 30.0\n",
      "Episode * 81 * Reward is ==> 34.0\n",
      "Episode * 82 * Reward is ==> 9.0\n",
      "Episode * 83 * Reward is ==> 51.0\n",
      "Episode * 84 * Reward is ==> 39.0\n",
      "Episode * 85 * Reward is ==> 23.0\n",
      "Episode * 86 * Reward is ==> 34.0\n",
      "Episode * 87 * Reward is ==> 21.0\n",
      "Episode * 88 * Reward is ==> 14.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ericp\\OneDrive\\Desktop\\Visual Studio Code Projekte\\Python Universität\\SOC_with_random_terminal_time\\OpenAI.ipynb Zelle 3\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ericp/OneDrive/Desktop/Visual%20Studio%20Code%20Projekte/Python%20Universit%C3%A4t/SOC_with_random_terminal_time/OpenAI.ipynb#W2sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m MC\u001b[39m.\u001b[39mbuffer\u001b[39m.\u001b[39mrecord((state,action,reward, new_state, done))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ericp/OneDrive/Desktop/Visual%20Studio%20Code%20Projekte/Python%20Universit%C3%A4t/SOC_with_random_terminal_time/OpenAI.ipynb#W2sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39mif\u001b[39;00m(MC\u001b[39m.\u001b[39mbuffer\u001b[39m.\u001b[39mbuffer_counter \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m MC\u001b[39m.\u001b[39mbatch_size):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/ericp/OneDrive/Desktop/Visual%20Studio%20Code%20Projekte/Python%20Universit%C3%A4t/SOC_with_random_terminal_time/OpenAI.ipynb#W2sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m     MC\u001b[39m.\u001b[39;49mlearn()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ericp/OneDrive/Desktop/Visual%20Studio%20Code%20Projekte/Python%20Universit%C3%A4t/SOC_with_random_terminal_time/OpenAI.ipynb#W2sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m     MC\u001b[39m.\u001b[39mupdate_target(MC\u001b[39m.\u001b[39mtarget_critic\u001b[39m.\u001b[39mvariables, MC\u001b[39m.\u001b[39mcritic\u001b[39m.\u001b[39mvariables)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ericp/OneDrive/Desktop/Visual%20Studio%20Code%20Projekte/Python%20Universit%C3%A4t/SOC_with_random_terminal_time/OpenAI.ipynb#W2sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m state \u001b[39m=\u001b[39m new_state\n",
      "\u001b[1;32mc:\\Users\\ericp\\OneDrive\\Desktop\\Visual Studio Code Projekte\\Python Universität\\SOC_with_random_terminal_time\\OpenAI.ipynb Zelle 3\u001b[0m in \u001b[0;36mMountainCar.learn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/ericp/OneDrive/Desktop/Visual%20Studio%20Code%20Projekte/Python%20Universit%C3%A4t/SOC_with_random_terminal_time/OpenAI.ipynb#W2sZmlsZQ%3D%3D?line=108'>109</a>\u001b[0m next_state_batch \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mconvert_to_tensor(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuffer\u001b[39m.\u001b[39mnext_state_buffer[batch_indices])\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/ericp/OneDrive/Desktop/Visual%20Studio%20Code%20Projekte/Python%20Universit%C3%A4t/SOC_with_random_terminal_time/OpenAI.ipynb#W2sZmlsZQ%3D%3D?line=109'>110</a>\u001b[0m done_batch \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mconvert_to_tensor(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuffer\u001b[39m.\u001b[39mdone_buffer[batch_indices])\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/ericp/OneDrive/Desktop/Visual%20Studio%20Code%20Projekte/Python%20Universit%C3%A4t/SOC_with_random_terminal_time/OpenAI.ipynb#W2sZmlsZQ%3D%3D?line=111'>112</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mupdate(state_batch, action_batch, reward_batch, next_state_batch,done_batch)\n",
      "\u001b[1;32mc:\\Users\\ericp\\OneDrive\\Desktop\\Visual Studio Code Projekte\\Python Universität\\SOC_with_random_terminal_time\\OpenAI.ipynb Zelle 3\u001b[0m in \u001b[0;36mMountainCar.update\u001b[1;34m(self, state_batch, action_batch, reward_batch, next_state_batch, done_batch)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ericp/OneDrive/Desktop/Visual%20Studio%20Code%20Projekte/Python%20Universit%C3%A4t/SOC_with_random_terminal_time/OpenAI.ipynb#W2sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m target_vals \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mreshape(target_vals, [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_size,\u001b[39m1\u001b[39m])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ericp/OneDrive/Desktop/Visual%20Studio%20Code%20Projekte/Python%20Universit%C3%A4t/SOC_with_random_terminal_time/OpenAI.ipynb#W2sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m y \u001b[39m=\u001b[39m reward_batch \u001b[39m+\u001b[39m (tf\u001b[39m.\u001b[39mones_like(done_batch,dtype \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mfloat32)\u001b[39m-\u001b[39mdone_batch)\u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgamma\u001b[39m*\u001b[39mtarget_vals\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/ericp/OneDrive/Desktop/Visual%20Studio%20Code%20Projekte/Python%20Universit%C3%A4t/SOC_with_random_terminal_time/OpenAI.ipynb#W2sZmlsZQ%3D%3D?line=76'>77</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcritic\u001b[39m.\u001b[39;49mfit(state_batch,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ericp/OneDrive/Desktop/Visual%20Studio%20Code%20Projekte/Python%20Universit%C3%A4t/SOC_with_random_terminal_time/OpenAI.ipynb#W2sZmlsZQ%3D%3D?line=77'>78</a>\u001b[0m y \u001b[39m=\u001b[39;49m y,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ericp/OneDrive/Desktop/Visual%20Studio%20Code%20Projekte/Python%20Universit%C3%A4t/SOC_with_random_terminal_time/OpenAI.ipynb#W2sZmlsZQ%3D%3D?line=78'>79</a>\u001b[0m verbose \u001b[39m=\u001b[39;49m \u001b[39m0\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ericp/OneDrive/Desktop/Visual%20Studio%20Code%20Projekte/Python%20Universit%C3%A4t/SOC_with_random_terminal_time/OpenAI.ipynb#W2sZmlsZQ%3D%3D?line=79'>80</a>\u001b[0m batch_size \u001b[39m=\u001b[39;49m  \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_size)\n",
      "File \u001b[1;32mc:\\Users\\ericp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 64\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\ericp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py:1372\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1369\u001b[0m data_handler\u001b[39m.\u001b[39m_initial_epoch \u001b[39m=\u001b[39m (  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   1370\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_load_initial_epoch_from_ckpt(initial_epoch))\n\u001b[0;32m   1371\u001b[0m logs \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1372\u001b[0m \u001b[39mfor\u001b[39;00m epoch, iterator \u001b[39min\u001b[39;00m data_handler\u001b[39m.\u001b[39menumerate_epochs():\n\u001b[0;32m   1373\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreset_metrics()\n\u001b[0;32m   1374\u001b[0m   callbacks\u001b[39m.\u001b[39mon_epoch_begin(epoch)\n",
      "File \u001b[1;32mc:\\Users\\ericp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\data_adapter.py:1191\u001b[0m, in \u001b[0;36mDataHandler.enumerate_epochs\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1189\u001b[0m \u001b[39m\"\"\"Yields `(epoch, tf.data.Iterator)`.\"\"\"\u001b[39;00m\n\u001b[0;32m   1190\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_truncate_execution_to_epoch():\n\u001b[1;32m-> 1191\u001b[0m   data_iterator \u001b[39m=\u001b[39m \u001b[39miter\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset)\n\u001b[0;32m   1192\u001b[0m   \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_initial_epoch, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_epochs):\n\u001b[0;32m   1193\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_insufficient_data:  \u001b[39m# Set by `catch_stop_iteration`.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ericp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:486\u001b[0m, in \u001b[0;36mDatasetV2.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    484\u001b[0m \u001b[39mif\u001b[39;00m context\u001b[39m.\u001b[39mexecuting_eagerly() \u001b[39mor\u001b[39;00m ops\u001b[39m.\u001b[39minside_function():\n\u001b[0;32m    485\u001b[0m   \u001b[39mwith\u001b[39;00m ops\u001b[39m.\u001b[39mcolocate_with(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variant_tensor):\n\u001b[1;32m--> 486\u001b[0m     \u001b[39mreturn\u001b[39;00m iterator_ops\u001b[39m.\u001b[39;49mOwnedIterator(\u001b[39mself\u001b[39;49m)\n\u001b[0;32m    487\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    488\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m`tf.data.Dataset` only supports Python-style \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    489\u001b[0m                      \u001b[39m\"\u001b[39m\u001b[39miteration in eager mode or within tf.function.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ericp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:755\u001b[0m, in \u001b[0;36mOwnedIterator.__init__\u001b[1;34m(self, dataset, components, element_spec)\u001b[0m\n\u001b[0;32m    751\u001b[0m   \u001b[39mif\u001b[39;00m (components \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m element_spec \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    752\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    753\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWhen `dataset` is provided, `element_spec` and `components` must \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    754\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mnot be specified.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 755\u001b[0m   \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_iterator(dataset)\n\u001b[0;32m    757\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_next_call_count \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\ericp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:787\u001b[0m, in \u001b[0;36mOwnedIterator._create_iterator\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    782\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    783\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterator_resource, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_deleter \u001b[39m=\u001b[39m (\n\u001b[0;32m    784\u001b[0m       gen_dataset_ops\u001b[39m.\u001b[39manonymous_iterator_v2(\n\u001b[0;32m    785\u001b[0m           output_types\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flat_output_types,\n\u001b[0;32m    786\u001b[0m           output_shapes\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flat_output_shapes))\n\u001b[1;32m--> 787\u001b[0m   gen_dataset_ops\u001b[39m.\u001b[39;49mmake_iterator(ds_variant, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_iterator_resource)\n\u001b[0;32m    788\u001b[0m   \u001b[39m# Delete the resource when this object is deleted\u001b[39;00m\n\u001b[0;32m    789\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_resource_deleter \u001b[39m=\u001b[39m IteratorResourceDeleter(\n\u001b[0;32m    790\u001b[0m       handle\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterator_resource,\n\u001b[0;32m    791\u001b[0m       deleter\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_deleter)\n",
      "File \u001b[1;32mc:\\Users\\ericp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py:3314\u001b[0m, in \u001b[0;36mmake_iterator\u001b[1;34m(dataset, iterator, name)\u001b[0m\n\u001b[0;32m   3312\u001b[0m \u001b[39mif\u001b[39;00m tld\u001b[39m.\u001b[39mis_eager:\n\u001b[0;32m   3313\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 3314\u001b[0m     _result \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_FastPathExecute(\n\u001b[0;32m   3315\u001b[0m       _ctx, \u001b[39m\"\u001b[39;49m\u001b[39mMakeIterator\u001b[39;49m\u001b[39m\"\u001b[39;49m, name, dataset, iterator)\n\u001b[0;32m   3316\u001b[0m     \u001b[39mreturn\u001b[39;00m _result\n\u001b[0;32m   3317\u001b[0m   \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "MC = MountainCar()\n",
    "\n",
    "ep_reward_list = []\n",
    "# To store average reward history of last few episodes\n",
    "avg_reward_list = []\n",
    "num_episode = 2000\n",
    "\n",
    "for ep in range(num_episode):\n",
    "    MC.env.reset()\n",
    "    done = False\n",
    "    state = MC.env.reset()\n",
    "    #state = tf.expand_dims(tf.convert_to_tensor(state),0)\n",
    "    state = np.reshape(state, [1,MC.state_dim])\n",
    "    episodic_reward = 0\n",
    "    t_counter = 0\n",
    "\n",
    "    if (ep % 100 == 0 and ep != 0):\n",
    "        MC.run_MC()\n",
    "    while(True):\n",
    "        MC.alpha = 0.0001\n",
    "        eps = np.max([0.05,0.1 * (1 / (1 +ep))])\n",
    "        action = MC.epsilon_greedy(state, eps)\n",
    "        \n",
    "        new_state, reward, done, info = MC.env.step(action)\n",
    "        \n",
    "        #new_state = tf.expand_dims(tf.convert_to_tensor(new_state.reshape(MC.state_dim)),0)\n",
    "        new_state = np.reshape(new_state, [1,MC.state_dim])\n",
    "        episodic_reward += reward\n",
    "        \n",
    "        MC.buffer.record((state,action,reward, new_state, done))\n",
    "        if(MC.buffer.buffer_counter >= MC.batch_size):\n",
    "            MC.learn()\n",
    "        \n",
    "            MC.update_target(MC.target_critic.variables, MC.critic.variables)\n",
    "\n",
    "        state = new_state\n",
    "        t_counter +=1\n",
    "        if (done):\n",
    "            break\n",
    "    ep_reward_list.append(episodic_reward)\n",
    "    # Mean of last 40 episodes\n",
    "    avg_reward = np.mean(ep_reward_list[-30:])\n",
    "    print(\"Episode * {} * AVG Reward is ==> {}\".format(ep, avg_reward))\n",
    "    avg_reward_list.append(avg_reward)\n",
    "# Plotting graph\n",
    "# Episodes versus Avg. Rewards\n",
    "plt.plot(ep_reward_list)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Avg. Epsiodic Reward\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import gym # for environment\n",
    "from collections import deque\n",
    "from keras.layers import Dense\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ericp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gym\\envs\\registration.py:505: UserWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1` with the environment ID `CartPole-v1`.\u001b[0m\n",
      "  logger.warn(\n",
      "c:\\Users\\ericp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\optimizer_v2\\adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0, time: 11\n",
      "episode: 1, time: 15\n",
      "episode: 2, time: 25\n",
      "episode: 3, time: 10\n",
      "episode: 4, time: 40\n",
      "episode: 5, time: 46\n",
      "episode: 6, time: 21\n",
      "episode: 7, time: 29\n",
      "episode: 8, time: 20\n",
      "episode: 9, time: 12\n",
      "episode: 10, time: 18\n",
      "episode: 11, time: 36\n",
      "episode: 12, time: 68\n",
      "episode: 13, time: 63\n",
      "episode: 14, time: 49\n",
      "episode: 15, time: 39\n",
      "episode: 16, time: 24\n",
      "episode: 17, time: 24\n",
      "episode: 18, time: 49\n",
      "episode: 19, time: 59\n",
      "episode: 20, time: 30\n",
      "episode: 21, time: 62\n",
      "episode: 22, time: 32\n",
      "episode: 23, time: 65\n",
      "episode: 24, time: 67\n",
      "episode: 25, time: 65\n",
      "episode: 26, time: 63\n",
      "episode: 27, time: 36\n",
      "episode: 28, time: 56\n",
      "episode: 29, time: 66\n",
      "episode: 30, time: 47\n",
      "episode: 31, time: 54\n",
      "episode: 32, time: 55\n",
      "episode: 33, time: 121\n",
      "episode: 34, time: 60\n",
      "episode: 35, time: 94\n",
      "episode: 36, time: 75\n",
      "episode: 37, time: 146\n",
      "episode: 38, time: 114\n",
      "episode: 39, time: 123\n",
      "episode: 40, time: 200\n",
      "episode: 41, time: 200\n",
      "episode: 42, time: 200\n",
      "episode: 43, time: 200\n",
      "episode: 44, time: 184\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ericp\\OneDrive\\Desktop\\Visual Studio Code Projekte\\Python Universität\\SOC_with_random_terminal_time\\OpenAI.ipynb Zelle 5\u001b[0m in \u001b[0;36m<cell line: 68>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ericp/OneDrive/Desktop/Visual%20Studio%20Code%20Projekte/Python%20Universit%C3%A4t/SOC_with_random_terminal_time/OpenAI.ipynb#W4sZmlsZQ%3D%3D?line=95'>96</a>\u001b[0m state \u001b[39m=\u001b[39m next_state\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ericp/OneDrive/Desktop/Visual%20Studio%20Code%20Projekte/Python%20Universit%C3%A4t/SOC_with_random_terminal_time/OpenAI.ipynb#W4sZmlsZQ%3D%3D?line=97'>98</a>\u001b[0m \u001b[39m# replay\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/ericp/OneDrive/Desktop/Visual%20Studio%20Code%20Projekte/Python%20Universit%C3%A4t/SOC_with_random_terminal_time/OpenAI.ipynb#W4sZmlsZQ%3D%3D?line=98'>99</a>\u001b[0m agent\u001b[39m.\u001b[39;49mreplay(batch_size)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/ericp/OneDrive/Desktop/Visual%20Studio%20Code%20Projekte/Python%20Universit%C3%A4t/SOC_with_random_terminal_time/OpenAI.ipynb#W4sZmlsZQ%3D%3D?line=100'>101</a>\u001b[0m \u001b[39m# adjust epsilon\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/ericp/OneDrive/Desktop/Visual%20Studio%20Code%20Projekte/Python%20Universit%C3%A4t/SOC_with_random_terminal_time/OpenAI.ipynb#W4sZmlsZQ%3D%3D?line=101'>102</a>\u001b[0m agent\u001b[39m.\u001b[39madaptiveEGreedy()\n",
      "\u001b[1;32mc:\\Users\\ericp\\OneDrive\\Desktop\\Visual Studio Code Projekte\\Python Universität\\SOC_with_random_terminal_time\\OpenAI.ipynb Zelle 5\u001b[0m in \u001b[0;36mDQLAgent.replay\u001b[1;34m(self, batch_size)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ericp/OneDrive/Desktop/Visual%20Studio%20Code%20Projekte/Python%20Universit%C3%A4t/SOC_with_random_terminal_time/OpenAI.ipynb#W4sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m     target \u001b[39m=\u001b[39m reward \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgamma \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39mamax(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mpredict(next_state)[\u001b[39m0\u001b[39m]) \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ericp/OneDrive/Desktop/Visual%20Studio%20Code%20Projekte/Python%20Universit%C3%A4t/SOC_with_random_terminal_time/OpenAI.ipynb#W4sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m     \u001b[39m# target = R(s,a) + gamma * max Q`(s`,a`)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ericp/OneDrive/Desktop/Visual%20Studio%20Code%20Projekte/Python%20Universit%C3%A4t/SOC_with_random_terminal_time/OpenAI.ipynb#W4sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m     \u001b[39m# target (max Q` value) is output of Neural Network which takes s` as an input \u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ericp/OneDrive/Desktop/Visual%20Studio%20Code%20Projekte/Python%20Universit%C3%A4t/SOC_with_random_terminal_time/OpenAI.ipynb#W4sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m     \u001b[39m# amax(): flatten the lists (make them 1 list) and take max value\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/ericp/OneDrive/Desktop/Visual%20Studio%20Code%20Projekte/Python%20Universit%C3%A4t/SOC_with_random_terminal_time/OpenAI.ipynb#W4sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m train_target \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mpredict(state) \u001b[39m# s --> NN --> Q(s,a)=train_target\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ericp/OneDrive/Desktop/Visual%20Studio%20Code%20Projekte/Python%20Universit%C3%A4t/SOC_with_random_terminal_time/OpenAI.ipynb#W4sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m train_target[\u001b[39m0\u001b[39m][action] \u001b[39m=\u001b[39m target\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ericp/OneDrive/Desktop/Visual%20Studio%20Code%20Projekte/Python%20Universit%C3%A4t/SOC_with_random_terminal_time/OpenAI.ipynb#W4sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mfit(state, train_target, verbose \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ericp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 64\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\ericp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py:1978\u001b[0m, in \u001b[0;36mModel.predict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1976\u001b[0m callbacks\u001b[39m.\u001b[39mon_predict_begin()\n\u001b[0;32m   1977\u001b[0m batch_outputs \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1978\u001b[0m \u001b[39mfor\u001b[39;00m _, iterator \u001b[39min\u001b[39;00m data_handler\u001b[39m.\u001b[39menumerate_epochs():  \u001b[39m# Single epoch.\u001b[39;00m\n\u001b[0;32m   1979\u001b[0m   \u001b[39mwith\u001b[39;00m data_handler\u001b[39m.\u001b[39mcatch_stop_iteration():\n\u001b[0;32m   1980\u001b[0m     \u001b[39mfor\u001b[39;00m step \u001b[39min\u001b[39;00m data_handler\u001b[39m.\u001b[39msteps():\n",
      "File \u001b[1;32mc:\\Users\\ericp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\data_adapter.py:1191\u001b[0m, in \u001b[0;36mDataHandler.enumerate_epochs\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1189\u001b[0m \u001b[39m\"\"\"Yields `(epoch, tf.data.Iterator)`.\"\"\"\u001b[39;00m\n\u001b[0;32m   1190\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_truncate_execution_to_epoch():\n\u001b[1;32m-> 1191\u001b[0m   data_iterator \u001b[39m=\u001b[39m \u001b[39miter\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset)\n\u001b[0;32m   1192\u001b[0m   \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_initial_epoch, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_epochs):\n\u001b[0;32m   1193\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_insufficient_data:  \u001b[39m# Set by `catch_stop_iteration`.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ericp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:486\u001b[0m, in \u001b[0;36mDatasetV2.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    484\u001b[0m \u001b[39mif\u001b[39;00m context\u001b[39m.\u001b[39mexecuting_eagerly() \u001b[39mor\u001b[39;00m ops\u001b[39m.\u001b[39minside_function():\n\u001b[0;32m    485\u001b[0m   \u001b[39mwith\u001b[39;00m ops\u001b[39m.\u001b[39mcolocate_with(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variant_tensor):\n\u001b[1;32m--> 486\u001b[0m     \u001b[39mreturn\u001b[39;00m iterator_ops\u001b[39m.\u001b[39;49mOwnedIterator(\u001b[39mself\u001b[39;49m)\n\u001b[0;32m    487\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    488\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m`tf.data.Dataset` only supports Python-style \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    489\u001b[0m                      \u001b[39m\"\u001b[39m\u001b[39miteration in eager mode or within tf.function.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ericp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:755\u001b[0m, in \u001b[0;36mOwnedIterator.__init__\u001b[1;34m(self, dataset, components, element_spec)\u001b[0m\n\u001b[0;32m    751\u001b[0m   \u001b[39mif\u001b[39;00m (components \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m element_spec \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    752\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    753\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWhen `dataset` is provided, `element_spec` and `components` must \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    754\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mnot be specified.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 755\u001b[0m   \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_iterator(dataset)\n\u001b[0;32m    757\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_next_call_count \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\ericp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:787\u001b[0m, in \u001b[0;36mOwnedIterator._create_iterator\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    782\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    783\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterator_resource, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_deleter \u001b[39m=\u001b[39m (\n\u001b[0;32m    784\u001b[0m       gen_dataset_ops\u001b[39m.\u001b[39manonymous_iterator_v2(\n\u001b[0;32m    785\u001b[0m           output_types\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flat_output_types,\n\u001b[0;32m    786\u001b[0m           output_shapes\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flat_output_shapes))\n\u001b[1;32m--> 787\u001b[0m   gen_dataset_ops\u001b[39m.\u001b[39;49mmake_iterator(ds_variant, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_iterator_resource)\n\u001b[0;32m    788\u001b[0m   \u001b[39m# Delete the resource when this object is deleted\u001b[39;00m\n\u001b[0;32m    789\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_resource_deleter \u001b[39m=\u001b[39m IteratorResourceDeleter(\n\u001b[0;32m    790\u001b[0m       handle\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterator_resource,\n\u001b[0;32m    791\u001b[0m       deleter\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_deleter)\n",
      "File \u001b[1;32mc:\\Users\\ericp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py:3314\u001b[0m, in \u001b[0;36mmake_iterator\u001b[1;34m(dataset, iterator, name)\u001b[0m\n\u001b[0;32m   3312\u001b[0m \u001b[39mif\u001b[39;00m tld\u001b[39m.\u001b[39mis_eager:\n\u001b[0;32m   3313\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 3314\u001b[0m     _result \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_FastPathExecute(\n\u001b[0;32m   3315\u001b[0m       _ctx, \u001b[39m\"\u001b[39;49m\u001b[39mMakeIterator\u001b[39;49m\u001b[39m\"\u001b[39;49m, name, dataset, iterator)\n\u001b[0;32m   3316\u001b[0m     \u001b[39mreturn\u001b[39;00m _result\n\u001b[0;32m   3317\u001b[0m   \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class DQLAgent(): \n",
    "    \n",
    "    def __init__(self, env):\n",
    "        # parameters and hyperparameters\n",
    "        \n",
    "        # this part is for neural network or build_model()\n",
    "        self.state_size = env.observation_space.shape[0] # this is for input of neural network node size\n",
    "        self.action_size = env.action_space.n # this is for out of neural network node size\n",
    "        \n",
    "        # this part is for replay()\n",
    "        self.gamma = 0.95\n",
    "        self.learning_rate = 0.001\n",
    "        \n",
    "        # this part is for adaptiveEGreedy()\n",
    "        self.epsilon = 1 # initial exploration rate\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.epsilon_min = 0.01\n",
    "        \n",
    "        self.memory = deque(maxlen = 1000) # a list with 1000 memory, if it becomes full first inputs will be deleted\n",
    "        \n",
    "        self.model = self.build_model()\n",
    "    \n",
    "    def build_model(self):\n",
    "        # neural network for deep Q learning\n",
    "        model = keras.Sequential()\n",
    "        model.add(Dense(48, input_dim = self.state_size, activation = 'tanh')) # first hidden layer\n",
    "        model.add(Dense(self.action_size, activation = 'linear')) # output layer\n",
    "        model.compile(loss = 'mse', optimizer = tf.keras.optimizers.Adam(lr = self.learning_rate))\n",
    "        return model\n",
    "    \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        # storage\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def act(self, state):\n",
    "        # acting, exploit or explore\n",
    "        if random.uniform(0,1) <= self.epsilon:\n",
    "            return env.action_space.sample()\n",
    "        else:\n",
    "            act_values = self.model.predict(state)\n",
    "            return np.argmax(act_values[0])\n",
    "            \n",
    "    \n",
    "    def replay(self, batch_size):\n",
    "        # training\n",
    "        \n",
    "        if len(self.memory) < batch_size:\n",
    "            return # memory is still not full\n",
    "        \n",
    "        minibatch = random.sample(self.memory, batch_size) # take 16 (batch_size) random samples from memory\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            if done: # if the game is over, I dont have next state, I just have reward \n",
    "                target = reward\n",
    "            else:\n",
    "                target = reward + self.gamma * np.amax(self.model.predict(next_state)[0]) \n",
    "                # target = R(s,a) + gamma * max Q`(s`,a`)\n",
    "                # target (max Q` value) is output of Neural Network which takes s` as an input \n",
    "                # amax(): flatten the lists (make them 1 list) and take max value\n",
    "            train_target = self.model.predict(state) # s --> NN --> Q(s,a)=train_target\n",
    "            train_target[0][action] = target\n",
    "            self.model.fit(state, train_target, verbose = 0) # verbose: dont show loss and epoch\n",
    "    \n",
    "    def adaptiveEGreedy(self):\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "            \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # initialize gym environment and agent\n",
    "    env = gym.make('CartPole-v0')\n",
    "    agent = DQLAgent(env)\n",
    "\n",
    "    batch_size = 16\n",
    "    episodes = 50\n",
    "    for e in range(episodes):\n",
    "        \n",
    "        # initialize environment\n",
    "        state = env.reset()\n",
    "        state = np.reshape(state, [1,4])\n",
    "        \n",
    "        time = 0 # each second I will get reward, because I want to sustain a balance forever\n",
    "        while True:\n",
    "            \n",
    "            # act\n",
    "            action = agent.act(state)\n",
    "            \n",
    "            # step\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            next_state = np.reshape(next_state, [1,4])\n",
    "            \n",
    "            # remember / storage\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            \n",
    "            # update state\n",
    "            state = next_state\n",
    "            \n",
    "            # replay\n",
    "            agent.replay(batch_size)\n",
    "            \n",
    "            # adjust epsilon\n",
    "            agent.adaptiveEGreedy()\n",
    "            \n",
    "            time += 1\n",
    "            \n",
    "            if done:\n",
    "                print('episode: {}, time: {}'.format(e, time))\n",
    "                break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369b169b6a5bc372c49943446f64d019decaea6189ec6cd1dc9e297757cbbf29"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
