{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import gym\n",
    "import pygame\n",
    "from keras import layers, optimizers, losses\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "DependencyNotInstalled",
     "evalue": "No module named 'mujoco_py'. (HINT: you need to install mujoco_py, and also perform the setup instructions here: https://github.com/openai/mujoco-py/.)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\ericp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gym\\envs\\mujoco\\mujoco_env.py:12\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 12\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mmujoco_py\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'mujoco_py'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mDependencyNotInstalled\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ericp\\OneDrive\\Desktop\\Visual Studio Code Projekte\\Python Universit√§t\\SOC_with_random_terminal_time\\OpenAI.ipynb Cell 2\u001b[0m in \u001b[0;36m<cell line: 175>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/ericp/OneDrive/Desktop/Visual%20Studio%20Code%20Projekte/Python%20Universit%C3%A4t/SOC_with_random_terminal_time/OpenAI.ipynb#W3sZmlsZQ%3D%3D?line=171'>172</a>\u001b[0m maxx \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/ericp/OneDrive/Desktop/Visual%20Studio%20Code%20Projekte/Python%20Universit%C3%A4t/SOC_with_random_terminal_time/OpenAI.ipynb#W3sZmlsZQ%3D%3D?line=173'>174</a>\u001b[0m start \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/ericp/OneDrive/Desktop/Visual%20Studio%20Code%20Projekte/Python%20Universit%C3%A4t/SOC_with_random_terminal_time/OpenAI.ipynb#W3sZmlsZQ%3D%3D?line=174'>175</a>\u001b[0m env \u001b[39m=\u001b[39m gym\u001b[39m.\u001b[39;49mmake(\u001b[39m\"\u001b[39;49m\u001b[39mInvertedDoublePendulum-v2\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/ericp/OneDrive/Desktop/Visual%20Studio%20Code%20Projekte/Python%20Universit%C3%A4t/SOC_with_random_terminal_time/OpenAI.ipynb#W3sZmlsZQ%3D%3D?line=175'>176</a>\u001b[0m \u001b[39m#env = gym.make(\"HumanoidFlagrunPyBulletEnv-v0\")\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/ericp/OneDrive/Desktop/Visual%20Studio%20Code%20Projekte/Python%20Universit%C3%A4t/SOC_with_random_terminal_time/OpenAI.ipynb#W3sZmlsZQ%3D%3D?line=176'>177</a>\u001b[0m \u001b[39m'''env.observation_space.shape'''\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ericp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gym\\envs\\registration.py:676\u001b[0m, in \u001b[0;36mmake\u001b[1;34m(id, **kwargs)\u001b[0m\n\u001b[0;32m    675\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmake\u001b[39m(\u001b[39mid\u001b[39m: \u001b[39mstr\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mEnv\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m--> 676\u001b[0m     \u001b[39mreturn\u001b[39;00m registry\u001b[39m.\u001b[39mmake(\u001b[39mid\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ericp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gym\\envs\\registration.py:520\u001b[0m, in \u001b[0;36mEnvRegistry.make\u001b[1;34m(self, path, **kwargs)\u001b[0m\n\u001b[0;32m    518\u001b[0m spec \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mspec(path)\n\u001b[0;32m    519\u001b[0m \u001b[39m# Construct the environment\u001b[39;00m\n\u001b[1;32m--> 520\u001b[0m \u001b[39mreturn\u001b[39;00m spec\u001b[39m.\u001b[39mmake(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ericp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gym\\envs\\registration.py:139\u001b[0m, in \u001b[0;36mEnvSpec.make\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    137\u001b[0m     env \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mentry_point(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m_kwargs)\n\u001b[0;32m    138\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 139\u001b[0m     \u001b[39mcls\u001b[39m \u001b[39m=\u001b[39m load(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mentry_point)\n\u001b[0;32m    140\u001b[0m     env \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m_kwargs)\n\u001b[0;32m    142\u001b[0m \u001b[39m# Make the environment aware of which spec it came from.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ericp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gym\\envs\\registration.py:55\u001b[0m, in \u001b[0;36mload\u001b[1;34m(name)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload\u001b[39m(name: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Type:\n\u001b[0;32m     54\u001b[0m     mod_name, attr_name \u001b[39m=\u001b[39m name\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m:\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 55\u001b[0m     mod \u001b[39m=\u001b[39m importlib\u001b[39m.\u001b[39;49mimport_module(mod_name)\n\u001b[0;32m     56\u001b[0m     fn \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(mod, attr_name)\n\u001b[0;32m     57\u001b[0m     \u001b[39mreturn\u001b[39;00m fn\n",
      "File \u001b[1;32mc:\\Users\\ericp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\importlib\\__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    124\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m    125\u001b[0m         level \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m--> 126\u001b[0m \u001b[39mreturn\u001b[39;00m _bootstrap\u001b[39m.\u001b[39;49m_gcd_import(name[level:], package, level)\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1050\u001b[0m, in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1006\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:688\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:883\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[1;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\ericp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gym\\envs\\mujoco\\__init__.py:1\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgym\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39menvs\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmujoco\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmujoco_env\u001b[39;00m \u001b[39mimport\u001b[39;00m MujocoEnv\n\u001b[0;32m      3\u001b[0m \u001b[39m# ^^^^^ so that user gets the correct error\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[39m# message if mujoco is not installed correctly\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgym\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39menvs\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmujoco\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mant\u001b[39;00m \u001b[39mimport\u001b[39;00m AntEnv\n",
      "File \u001b[1;32mc:\\Users\\ericp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gym\\envs\\mujoco\\mujoco_env.py:14\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mmujoco_py\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m---> 14\u001b[0m     \u001b[39mraise\u001b[39;00m error\u001b[39m.\u001b[39mDependencyNotInstalled(\n\u001b[0;32m     15\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m. (HINT: you need to install mujoco_py, and also perform the setup instructions here: https://github.com/openai/mujoco-py/.)\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m     16\u001b[0m             e\n\u001b[0;32m     17\u001b[0m         )\n\u001b[0;32m     18\u001b[0m     )\n\u001b[0;32m     20\u001b[0m DEFAULT_SIZE \u001b[39m=\u001b[39m \u001b[39m500\u001b[39m\n\u001b[0;32m     23\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconvert_observation_to_space\u001b[39m(observation):\n",
      "\u001b[1;31mDependencyNotInstalled\u001b[0m: No module named 'mujoco_py'. (HINT: you need to install mujoco_py, and also perform the setup instructions here: https://github.com/openai/mujoco-py/.)"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import tensorflow as tf\n",
    "from collections import deque\n",
    "  # register PyBullet enviroments with open ai gym\n",
    "import time\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "\n",
    "class SD3(object):\n",
    "    def __init__(self, action_size, state_size):\n",
    "        self.action_space = action_size\n",
    "        self.state_space = state_size\n",
    "        self.act_range = 1.0\n",
    "        self.actor_opt = tf.keras.optimizers.Adam(lr=1e-3)\n",
    "        self.critic_opt = tf.keras.optimizers.Adam(lr=1e-3)\n",
    "        self.q1 = self.make_critic()\n",
    "        self.q2 = self.make_critic()\n",
    "        self.q1_target = self.make_critic()\n",
    "        self.q2_target = self.make_critic()\n",
    "        self.p1 = self.make_actor()\n",
    "        self.p1_target = self.make_actor()\n",
    "        self.p2 = self.make_actor()\n",
    "        self.p2_target = self.make_actor()\n",
    "        #self.policy.summary()\n",
    "        #self.q1.summary()\n",
    "        self.move_weights()\n",
    "        self.buff = int(1e6)\n",
    "        self.states = np.zeros((self.buff, self.state_space[0]))\n",
    "        self.actions = np.zeros((self.buff, self.action_space[0]))\n",
    "        self.rewards = np.zeros((self.buff, 1))\n",
    "        self.dones = np.zeros((self.buff, 1))\n",
    "        self.next_states = np.zeros((self.buff, self.state_space[0]))\n",
    "        self.counter = 0\n",
    "        self.batch = 100\n",
    "        self.beta = 0.5\n",
    "        self.gamma = 0.99 \n",
    "        self.tau = 0.005\n",
    "        self.p_noise = 0.2 * self.act_range\n",
    "        self.t_noise = 0.5 * self.act_range\n",
    "        self.noise_sample = 50\n",
    "        self.mean = 0\n",
    "        self.std = 0.1\n",
    "     \n",
    "    def make_critic(self):\n",
    "        state_ = tf.keras.layers.Input(shape=(self.state_space[0]))\n",
    "        action_ = tf.keras.layers.Input(shape=(self.action_space[0]))\n",
    "        state = tf.keras.layers.Dense(256, activation='relu', name='state1')(state_)\n",
    "        action = tf.keras.layers.Dense(64, activation='relu', name='act1')(action_)\n",
    "        x = tf.keras.layers.Concatenate()([state, action])\n",
    "        x = tf.keras.layers.Dense(256, activation='relu', name='dense2')(x)\n",
    "        x = tf.keras.layers.Dense(256, activation='relu', name='dense3')(x)\n",
    "        x = tf.keras.layers.Dense(1, name='output')(x)\n",
    "        model = tf.keras.models.Model(inputs=[state_, action_], outputs=x)\n",
    "        return model\n",
    "    \n",
    "    def make_actor(self):\n",
    "        last_init = tf.random_uniform_initializer(minval=-0.003, maxval=0.003)\n",
    "        state_ = tf.keras.layers.Input(shape=(self.state_space[0]))\n",
    "        x = tf.keras.layers.Dense(128, activation='relu', name='dense1')(state_)\n",
    "        x = tf.keras.layers.Dense(512, activation='relu', name='dense2')(x)\n",
    "        x = tf.keras.layers.Dense(128, activation='relu', name='dense3')(x)\n",
    "        x = tf.keras.layers.Dense(self.action_space[0], activation='tanh', name='output', kernel_initializer=last_init)(x)\n",
    "        x = x * self.act_range\n",
    "        model = tf.keras.models.Model(inputs=state_, outputs=x)\n",
    "        return model\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, _):\n",
    "        i = self.counter % self.buff\n",
    "        self.states[i] = state\n",
    "        self.actions[i] = action\n",
    "        self.rewards[i] = reward\n",
    "        self.next_states[i] = next_state\n",
    "        self.dones[i] = int(done)\n",
    "        self.counter += 1\n",
    "\n",
    "    def move_weights(self):\n",
    "        self.q1_target.set_weights(self.q1.get_weights())\n",
    "        self.q2_target.set_weights(self.q2.get_weights())\n",
    "        self.p1_target.set_weights(self.p1.get_weights())\n",
    "        self.p2_target.set_weights(self.p2.get_weights())\n",
    "\n",
    "    def get_action(self, obs):\n",
    "        obs = np.array([obs])\n",
    "        act1 = self.p1(obs).numpy()\n",
    "        act2 = self.p2(obs).numpy()\n",
    "        val1 = tf.squeeze(self.q1([obs, act1])).numpy()\n",
    "        val2 = tf.squeeze(self.q2([obs, act2])).numpy()\n",
    "        action = act1 if val1 >= val2 else act2\n",
    "        action = np.clip(action, -self.act_range, self.act_range)\n",
    "        return action[0]\n",
    "        #return [np.squeeze(action)]\n",
    "\n",
    "    def train(self):\n",
    "        self.update(self.p1, self.p1_target, self.q1, self.q1_target)\n",
    "        self.update(self.p2, self.p2_target, self.q2, self.q2_target)\n",
    "\n",
    "    def softmax_operator(self, qvals):\n",
    "        max_q = tf.math.reduce_max(qvals, axis=1, keepdims=True)\n",
    "\n",
    "        norm_q_vals = qvals - max_q\n",
    "        e_beta_normQ = tf.math.exp(self.beta * norm_q_vals)\n",
    "        Q_mult_e = qvals * e_beta_normQ\n",
    "\n",
    "        numerators = Q_mult_e\n",
    "        denominators = e_beta_normQ\n",
    "\n",
    "        sum_numerators = tf.math.reduce_sum(numerators, axis=1)\n",
    "        sum_denominators = tf.math.reduce_sum(denominators, axis=1)\n",
    "        softmax_q_vals = sum_numerators / sum_denominators\n",
    "        softmax_q_vals = tf.expand_dims(softmax_q_vals, axis=1)\n",
    "        return softmax_q_vals\n",
    "\n",
    "\n",
    "    def update(self, p, p_t, q, q_t):\n",
    "        batch_indices = np.random.choice(min(self.counter, self.buff), self.batch)\n",
    "        state_batch = tf.convert_to_tensor(self.states[batch_indices])\n",
    "        action_batch = tf.convert_to_tensor(self.actions[batch_indices])\n",
    "        reward_batch = tf.convert_to_tensor(self.rewards[batch_indices])\n",
    "        reward_batch = tf.cast(reward_batch, dtype=tf.float32)\n",
    "        next_state_batch = tf.convert_to_tensor(self.next_states[batch_indices])\n",
    "        dones_batch = tf.convert_to_tensor(self.dones[batch_indices])\n",
    "        dones_batch = tf.cast(dones_batch, dtype=tf.float32)\n",
    "        # Train critic\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            next_action = p_t(next_state_batch)\n",
    "            next_action = tf.expand_dims(next_action, axis=1)\n",
    "            noise = tf.random.normal((action_batch.shape[0], self.noise_sample, action_batch.shape[1]))\n",
    "            noise *= self.p_noise\n",
    "            noise = tf.clip_by_value(noise, -self.t_noise, self.t_noise)\n",
    "            next_action = tf.clip_by_value(noise + next_action, -self.act_range, self.act_range)\n",
    "            next_state = tf.expand_dims(next_state_batch, axis=1)\n",
    "            next_state = tf.repeat(next_state, repeats=[self.noise_sample], axis=1)\n",
    "\n",
    "\n",
    "            next_Q1 = self.q1_target([next_state, next_action])\n",
    "            next_Q2 = self.q2_target([next_state, next_action])\n",
    "            next_Q = tf.math.minimum(next_Q1, next_Q2)\n",
    "            next_Q = tf.squeeze(next_Q)\n",
    "\n",
    "            softmax_next_Q = self.softmax_operator(next_Q)\n",
    "            next_Q = softmax_next_Q\n",
    "            target_Q = reward + (1 - dones_batch) * self.gamma * next_Q\n",
    "            current_Q = q([state_batch, action_batch], training=True)\n",
    "            msbe = tf.math.reduce_mean(tf.math.square(current_Q - target_Q))\n",
    "            current_act = p(state_batch)\n",
    "            policy_loss = -q([state_batch, current_act])\n",
    "        \n",
    "        critic_gradients = tape.gradient(msbe, q.trainable_variables)\n",
    "        self.critic_opt.apply_gradients(zip(critic_gradients, q.trainable_variables))\n",
    "        policy_gradients = tape.gradient(policy_loss, p.trainable_variables)\n",
    "        self.actor_opt.apply_gradients(zip(policy_gradients, p.trainable_variables))\n",
    "\n",
    "        self.update_target(p_t.trainable_variables, p.trainable_variables)\n",
    "        self.update_target(q_t.trainable_variables, q.trainable_variables)\n",
    "    \n",
    "    @tf.function\n",
    "    def update_target(self, target_weights, weights):\n",
    "        for (a, b) in zip(target_weights, weights):\n",
    "            a.assign(b * self.tau + a * (1 - self.tau))\n",
    "\n",
    "# Hyperparameters\n",
    "steps = int(1e6)\n",
    "#steps = 50000\n",
    "windows = 50\n",
    "learn_delay = int(1e4)\n",
    "#learn_delay = 1000\n",
    "#learn_delay = 100\n",
    "minn = -1\n",
    "maxx = 1\n",
    "\n",
    "start = time.time()\n",
    "env = gym.make(\"Pendulum-v1\")\n",
    "#env = gym.make(\"HumanoidFlagrunPyBulletEnv-v0\")\n",
    "'''env.observation_space.shape'''\n",
    "print(env.action_space, env.action_space.shape)\n",
    "print(env.observation_space, env.observation_space.shape)\n",
    "#input()\n",
    "agent = SD3(env.action_space.shape, env.observation_space.shape)\n",
    "rewards = []\n",
    "avg_reward = deque(maxlen=steps)\n",
    "best_avg_reward = -math.inf\n",
    "rs = deque(maxlen=windows)\n",
    "i = 0\n",
    "step = 0\n",
    "while True:\n",
    "    s1 = env.reset()\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    while not done:\n",
    "        #env.render()\n",
    "        if step < learn_delay:\n",
    "            action = (maxx - minn) * np.random.random(env.action_space.shape) + minn\n",
    "        else:\n",
    "            action = agent.get_action(s1)\n",
    "        s2, reward, done, info = env.step(action)\n",
    "        total_reward += reward\n",
    "        agent.remember(s1, action, reward, s2, done)\n",
    "        if agent.counter > learn_delay:\n",
    "            agent.train()\n",
    "        s1 = s2\n",
    "        step += 1\n",
    "    rs.append(total_reward)\n",
    "    avg = np.mean(rs)\n",
    "    avg_reward.append(avg)\n",
    "    if avg > best_avg_reward:\n",
    "        best_avg_reward = avg\n",
    "    \n",
    "    print(\"\\rStep {}/{} Iteration {} || Best average reward {}, Current Average {}, Current Iteration Reward {}\".format(step, steps, i, best_avg_reward, avg, total_reward), end='', flush=True)\n",
    "    i += 1\n",
    "    if step >= steps:\n",
    "        break\n",
    "\n",
    "print(time.time() - start)\n",
    "#np.save(\"rewards\", np.asarray(rewards))\n",
    "#np.save(\"double_pen_sd3_0\", np.asarray(avg_reward))\n",
    "plt.plot(rewards, color='olive', label='Reward')\n",
    "plt.plot(avg_reward, color='red', label='Average')\n",
    "plt.legend()\n",
    "#plt.title(\"Lunar Lander\")\n",
    "plt.ylabel('Reward')\n",
    "plt.xlabel('Step')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class experience_memory():\n",
    "\n",
    "    def __init__(self, buffer_capacity, batch_size, state_dim, action_dim):\n",
    "        # Number of \"experiences\" to store at max\n",
    "        self.buffer_capacity = buffer_capacity\n",
    "        # Num of tuples to train on.\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # Its tells us num of times record() was called.\n",
    "        self.buffer_counter = 0\n",
    "\n",
    "        # Instead of list of tuples as the exp.replay concept go\n",
    "        # We use different np.arrays for each tuple element\n",
    "        self.state_buffer = np.zeros((self.buffer_capacity, state_dim), dtype=np.float32)\n",
    "        self.action_buffer = np.zeros((self.buffer_capacity, action_dim), dtype=np.int32)\n",
    "        self.reward_buffer = np.zeros((self.buffer_capacity, 1), dtype=np.float32)\n",
    "        self.next_state_buffer = np.zeros((self.buffer_capacity, state_dim), dtype=np.float32)\n",
    "        self.done_buffer = np.zeros((self.buffer_capacity, 1), dtype=np.float32)\n",
    "\n",
    "    # Takes (s,a,r,s') obervation tuple as input\n",
    "    def record(self, obs_tuple):\n",
    "        # Set index to zero if buffer_capacity is exceeded,\n",
    "        # replacing old records\n",
    "        index = self.buffer_counter % self.buffer_capacity\n",
    "\n",
    "        self.state_buffer[index] = obs_tuple[0]\n",
    "        self.action_buffer[index] = obs_tuple[1]\n",
    "        self.reward_buffer[index] = obs_tuple[2]\n",
    "        self.next_state_buffer[index] = obs_tuple[3]\n",
    "        self.done_buffer[index] = obs_tuple[4]\n",
    "\n",
    "\n",
    "        self.buffer_counter += 1\n",
    "\n",
    "class MountainCar():\n",
    "\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "        self.batch_size = 32\n",
    "        self.max_memory_size = 500000\n",
    "\n",
    "        self.state_dim = 4\n",
    "        self.action_dim = 1\n",
    "\n",
    "        self.gamma = 0.99\n",
    "        self.tau = 0.001\n",
    "        self.lower_action_bound = -1\n",
    "        self.upper_action_bound = 1\n",
    "\n",
    "        self.action_space = np.array([0,1])\n",
    "        self.num_a = len(self.action_space)\n",
    "\n",
    "        self.buffer = experience_memory(self.max_memory_size, self.batch_size, self.state_dim, self.action_dim)\n",
    "\n",
    "        # init the neural netsf\n",
    "        self.critic = self.get_critic_NN()\n",
    "        self.target_critic = self.get_critic_NN()\n",
    "        self.target_critic.set_weights(self.critic.get_weights())\n",
    "        self.eps = 1\n",
    "        self.alpha = 0.001\n",
    "        self.critic_optimizer = tf.keras.optimizers.Adam(self.alpha)\n",
    "\n",
    "    @tf.function\n",
    "    def update(self, state_batch, action_batch, reward_batch, next_state_batch, done_batch):\n",
    "             \n",
    "        \n",
    "        next_q_vals =  self.target_critic(next_state_batch)\n",
    "            \n",
    "        target_vals = tf.reshape(tf.reduce_max(next_q_vals, axis =1),[self.batch_size,1])\n",
    "        y = reward_batch + (1-done_batch)* self.gamma*target_vals\n",
    "        y = tf.reshape(y,[self.batch_size,])\n",
    "        mask = tf.reshape(tf.one_hot(action_batch, self.num_a, dtype=tf.float32),[self.batch_size, self.num_a])\n",
    "        with tf.GradientTape() as tape:\n",
    "            \n",
    "            critic_value = self.critic(state_batch)\n",
    "      \n",
    "            critic_pred = tf.reduce_sum(tf.multiply(critic_value,mask), axis=1)\n",
    "          \n",
    "            #critic_loss = tf.reduce_mean(tf.math.square(y-critic_value))\n",
    "            critic_loss = losses.MSE(y,critic_pred)\n",
    "        \n",
    "        \n",
    "        critic_grad = tape.gradient(critic_loss, self.critic.trainable_variables)\n",
    "        critic_grad, _ = tf.clip_by_global_norm(critic_grad, 2.0)\n",
    "        self.critic_optimizer.apply_gradients(zip(critic_grad, self.critic.trainable_variables))\n",
    "\n",
    "        \n",
    "    def learn(self):\n",
    "        # get sample\n",
    "\n",
    "        record_range = min(self.buffer.buffer_counter, self.buffer.buffer_capacity)\n",
    "\n",
    "        batch_indices = np.random.choice(record_range, self.batch_size)\n",
    "        \n",
    "        state_batch = tf.convert_to_tensor(self.buffer.state_buffer[batch_indices])\n",
    "        action_batch = tf.convert_to_tensor(self.buffer.action_buffer[batch_indices])\n",
    "        reward_batch = tf.convert_to_tensor(self.buffer.reward_buffer[batch_indices])\n",
    "        next_state_batch = tf.convert_to_tensor(self.buffer.next_state_buffer[batch_indices])\n",
    "        done_batch = tf.convert_to_tensor(self.buffer.done_buffer[batch_indices])\n",
    "        \n",
    "        self.update(state_batch, action_batch, reward_batch, next_state_batch,done_batch)\n",
    "\n",
    "    @tf.function\n",
    "    def update_target(self, target_weights, weights):\n",
    "        for (a,b) in zip(target_weights, weights):\n",
    "            a.assign(self.tau *b + (1-self.tau) *a)\n",
    "\n",
    "    def get_critic_NN(self):\n",
    "        # input [state, action]\n",
    "        \n",
    "        state_input = layers.Input(shape =(self.state_dim,))\n",
    "\n",
    "        out = layers.Dense(32, activation = 'relu')(state_input)\n",
    "        out = layers.BatchNormalization()(out)\n",
    "        out = layers.Dense(32, activation = 'relu')(out)\n",
    "        out = layers.Dense(self.num_a)(out)\n",
    "\n",
    "        \n",
    "        model = keras.Model(inputs = state_input, outputs = out)\n",
    "        model.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(learning_rate= 0.001),\n",
    "            loss=tf.keras.losses.MeanSquaredError(),\n",
    "            metrics=[tf.keras.metrics.MeanSquaredError()],\n",
    "        )\n",
    "        return model\n",
    "\n",
    "    def epsilon_greedy(self, state, eps):\n",
    "\n",
    "        q_vals = self.critic(state)\n",
    "        \n",
    "        if (eps > np.random.rand()):\n",
    "           \n",
    "            rand_ind = np.random.choice(self.num_a)\n",
    "            \n",
    "            return self.action_space[rand_ind]\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            a_ind = tf.argmax(q_vals,axis = 1)[0]\n",
    "            \n",
    "            return self.action_space[a_ind]\n",
    "    \n",
    "    def run_MC(self):\n",
    "        done = False\n",
    "        state = self.env.reset()\n",
    "        state = tf.expand_dims(tf.convert_to_tensor(state),0)\n",
    "        t_counter = 0\n",
    "        while (True):\n",
    "            self.env.render()\n",
    "            time.sleep(0.01)\n",
    "            \n",
    "            a_ind = np.argmax(self.critic(state))\n",
    "            \n",
    "            action = self.action_space[a_ind]\n",
    "            new_state, reward, done, info = self.env.step(action)\n",
    "        \n",
    "            new_state = tf.expand_dims(tf.convert_to_tensor(new_state.reshape(self.state_dim)),0)\n",
    "            state = new_state\n",
    "            t_counter += 1\n",
    "\n",
    "            if (done):\n",
    "                break\n",
    "        print('Zeit: ', t_counter)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([ 0.03865113, -0.0289197 ,  0.0269503 , -0.02833511], dtype=float32), {})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Users\\pillieri\\Desktop\\Visual Studio Code Projekte\\Python Projekte\\.venv\\lib\\site-packages\\numpy\\core\\fromnumeric.py:43: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  result = getattr(asarray(obj), method)(*args, **kwds)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 2 into shape (1,4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [3], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[39m#state = tf.expand_dims(tf.convert_to_tensor(state),0)\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[39mprint\u001b[39m(state)\n\u001b[1;32m---> 14\u001b[0m state \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mreshape(state, [\u001b[39m1\u001b[39;49m,MC\u001b[39m.\u001b[39;49mstate_dim])\n\u001b[0;32m     15\u001b[0m episodic_reward \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m     16\u001b[0m t_counter \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mreshape\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32md:\\Users\\pillieri\\Desktop\\Visual Studio Code Projekte\\Python Projekte\\.venv\\lib\\site-packages\\numpy\\core\\fromnumeric.py:298\u001b[0m, in \u001b[0;36mreshape\u001b[1;34m(a, newshape, order)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[39m@array_function_dispatch\u001b[39m(_reshape_dispatcher)\n\u001b[0;32m    199\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mreshape\u001b[39m(a, newshape, order\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mC\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m    200\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    201\u001b[0m \u001b[39m    Gives a new shape to an array without changing its data.\u001b[39;00m\n\u001b[0;32m    202\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    296\u001b[0m \u001b[39m           [5, 6]])\u001b[39;00m\n\u001b[0;32m    297\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 298\u001b[0m     \u001b[39mreturn\u001b[39;00m _wrapfunc(a, \u001b[39m'\u001b[39;49m\u001b[39mreshape\u001b[39;49m\u001b[39m'\u001b[39;49m, newshape, order\u001b[39m=\u001b[39;49morder)\n",
      "File \u001b[1;32md:\\Users\\pillieri\\Desktop\\Visual Studio Code Projekte\\Python Projekte\\.venv\\lib\\site-packages\\numpy\\core\\fromnumeric.py:54\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     52\u001b[0m bound \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(obj, method, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m     53\u001b[0m \u001b[39mif\u001b[39;00m bound \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m---> 54\u001b[0m     \u001b[39mreturn\u001b[39;00m _wrapit(obj, method, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m     56\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     57\u001b[0m     \u001b[39mreturn\u001b[39;00m bound(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n",
      "File \u001b[1;32md:\\Users\\pillieri\\Desktop\\Visual Studio Code Projekte\\Python Projekte\\.venv\\lib\\site-packages\\numpy\\core\\fromnumeric.py:43\u001b[0m, in \u001b[0;36m_wrapit\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m:\n\u001b[0;32m     42\u001b[0m     wrap \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m---> 43\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(asarray(obj), method)(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m     44\u001b[0m \u001b[39mif\u001b[39;00m wrap:\n\u001b[0;32m     45\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(result, mu\u001b[39m.\u001b[39mndarray):\n",
      "\u001b[1;31mValueError\u001b[0m: cannot reshape array of size 2 into shape (1,4)"
     ]
    }
   ],
   "source": [
    "MC = MountainCar()\n",
    "\n",
    "ep_reward_list = []\n",
    "# To store average reward history of last few episodes\n",
    "avg_reward_list = []\n",
    "num_episode = 5000\n",
    "decay = 0.9999\n",
    "for ep in range(num_episode):\n",
    "    \n",
    "    done = False\n",
    "    state = MC.env.reset()\n",
    "    #state = tf.expand_dims(tf.convert_to_tensor(state),0)\n",
    "    \n",
    "    state = np.reshape(state, [1,MC.state_dim])\n",
    "    episodic_reward = 0\n",
    "    t_counter = 0\n",
    "\n",
    "    #if (ep % 100 == 0 and ep != 0):\n",
    "        #MC.run_MC()\n",
    "    while(t_counter < 200):\n",
    "        MC.alpha = 0.0001  #np.max([0.0001,0.01 * (1 / (1 +ep))])\n",
    "        MC.eps = np.max([0.01, MC.eps * decay])\n",
    "        action = MC.epsilon_greedy(state, MC.eps)\n",
    "        \n",
    "        new_state, reward, done, info = MC.env.step(action)\n",
    "        \n",
    "        #new_state = tf.expand_dims(tf.convert_to_tensor(new_state.reshape(MC.state_dim)),0)\n",
    "       \n",
    "        new_state = np.reshape(new_state, [1,MC.state_dim])\n",
    "        episodic_reward += reward\n",
    "        \n",
    "        MC.buffer.record((state,action,reward, new_state, done))\n",
    "       \n",
    "        MC.learn()\n",
    "    \n",
    "        MC.update_target(MC.target_critic.variables, MC.critic.variables)\n",
    "\n",
    "        state = new_state\n",
    "        t_counter +=1\n",
    "        if (done):\n",
    "            break\n",
    "    ep_reward_list.append(episodic_reward)\n",
    "    # Mean of last 40 episodes\n",
    "    avg_reward = np.mean(ep_reward_list[-20:])\n",
    "    print(\"Episode * {} * AVG Reward is ==> {}, eps ==> {}\".format(ep, avg_reward, MC.eps))\n",
    "    avg_reward_list.append(avg_reward)\n",
    "# Plotting graph\n",
    "# Episodes versus Avg. Rewards\n",
    "plt.plot(avg_reward_list)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Avg. Epsiodic Reward\")\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369b169b6a5bc372c49943446f64d019decaea6189ec6cd1dc9e297757cbbf29"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
